[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CNU Deep Learning",
    "section": "",
    "text": "1 CNU Cell-free & Deep learning",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "CNU Deep Learning",
    "section": "1.1 Welcome",
    "text": "1.1 Welcome\n\n2024.5 충남대학교, 기계학습 기반 서열 분석 강의 노트\nHaseong Kim (at KRIBB)\nChatGTP로 기본 contents를 만들고 검토, 보완하며 작성된 노트\n[User] 태그가 붙은 파란색 텍스트가 프롬프트이며 이 후 [ChatGTP] 태그가 GTP (4.0) 답변\nGPT에 질의한 프롬프트는 초안 수준으로 프롬프트를 정교하게 보강해서 질의할 경우 더 높은 수준의 답변을 얻을 수 있음",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#환경",
    "href": "index.html#환경",
    "title": "CNU Deep Learning",
    "section": "1.2 환경",
    "text": "1.2 환경\n\n실습 환경은 colab을 활용하며 파일 저장 등은 구글 드라이브를 활용함\n본 github repository에서 ipynb 파일을 다운로드 해서 구글 드라이브 임의의 디렉토리에 (2024-cnu-lecture) 복사 후 활용",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "cnu_introduction.html",
    "href": "cnu_introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 [User]\nI want to learn python and use it for molecular biology research such as design of library sequence, design of experiments, read high-throughput sequencing file, apply deep learning algorithm. What topics do i need to learn?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "cnu_introduction.html#chatgpt",
    "href": "cnu_introduction.html#chatgpt",
    "title": "2  Introduction",
    "section": "2.2 [ChatGPT]",
    "text": "2.2 [ChatGPT]\nLearning Python for molecular biology research involves a blend of general programming skills and domain-specific knowledge. Here’s a structured guide on the topics you need to cover:\n\n2.2.1 Table of Contents\n\n2.2.1.1 Basic Python Programming\n\nPython Syntax and Semantics: Learn basic syntax, data types (strings, lists, dictionaries), control structures (if statements, loops), functions, and error handling.\nModules and Packages: Understand how to import and use standard libraries and third-party packages.\nFile I/O: Learn how to read from and write to files.\n\n\n\n2.2.1.2 Data Handling and Analysis\n\nNumPy: For numerical operations and handling arrays.\nPandas: For data manipulation and analysis, essential for handling tabular data, such as gene expression matrices.\nMatplotlib/Seaborn: For data visualization to generate plots and graphs.\n\n\n\n2.2.1.3 Bioinformatics Libraries\n\nBiopython: Essential for biological computation. Learn how to read/write sequence files, perform sequence analysis, and interact with biological databases.\nScikit-bio: For biological analysis including sequence alignment, biodiversity analysis, etc.\n\n\n\n2.2.1.4 Sequence Analysis\n\nFASTA/FASTQ Parsing: Learn how to parse sequence files.\nSequence Alignment: Understand the basics of sequence alignment (global and local), and tools like BLAST.\nMotif Finding: Learn how to identify sequence motifs.\n\n\n\n2.2.1.5 High-Throughput Sequencing Data\n\nHTSeq: For reading and analyzing high-throughput sequencing data.\nPySAM: For reading and manipulating SAM/BAM files (aligned sequencing reads).\nSeqIO: Part of Biopython for input/output of sequence data.\n\n\n\n2.2.1.6 Experimental Design\n\nPrimer Design: Understand the principles of primer design and tools available (like Primer3) and their Python interfaces.\nGolden Gate Assembly and Gibson Assembly: Learn the principles and how to design experiments programmatically.\n\n\n\n2.2.1.7 Statistical Analysis\n\nSciPy: For scientific and technical computing.\nStatsmodels: For statistical modeling.\nR Integration: Learn how to use R from Python (via rpy2) if you need to use specific R packages for statistical analysis.\n\n\n\n2.2.1.8 Machine Learning and Deep Learning\n\nScikit-learn: For machine learning algorithms.\nTensorFlow/Keras or PyTorch: For deep learning.\nBioinformatics-specific models: Learn about models specific to bioinformatics, such as predicting protein structures or gene expression levels.\n\n\n\n2.2.1.9 Workflow Automation\n\nSnakemake: For workflow management.\nNextflow: Another workflow management tool, useful for reproducible research.\n\n\n\n2.2.1.10 Advanced Topics\n\nData Integration: Integrating different types of biological data (genomic, transcriptomic, proteomic).\nNetwork Biology: Understanding biological networks (e.g., protein-protein interaction networks).\nStructural Bioinformatics: Analyzing protein structures using tools like PyMOL.\n\n\n\n2.2.1.11 Learning Resources\n\nBooks\n\n“Python for Biologists” by Martin Jones\n“Bioinformatics Programming Using Python” by Mitchell L. Model\n\nOnline Courses\n\nCoursera: “Python for Genomic Data Science” by Johns Hopkins University\nedX: “Python for Data Science” by Microsoft\nBiostars: Online forum for bioinformatics questions and tutorials.\n\nTutorials and Documentation\n\nBiopython Tutorial and Cookbook: Comprehensive guide to using Biopython.\nNumPy and Pandas documentation: Essential for data manipulation.\nScikit-learn and TensorFlow/Keras documentation: For machine learning and deep learning.\n\n\nBy following this structured guide, you’ll be able to leverage Python effectively for molecular biology research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "cnu_introduction.html#user-1",
    "href": "cnu_introduction.html#user-1",
    "title": "2  Introduction",
    "section": "2.3 [User]",
    "text": "2.3 [User]\n I’ll learn it using google colab. can you let me know how to setup the colab environment for this learning",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "cnu_introduction.html#chatgpt-1",
    "href": "cnu_introduction.html#chatgpt-1",
    "title": "2  Introduction",
    "section": "2.4 [ChatGPT]",
    "text": "2.4 [ChatGPT]\nCertainly! Google Colab is an excellent platform for learning and running Python code, especially for bioinformatics. It provides a cloud-based Jupyter notebook environment that comes pre-configured with many scientific libraries. Here’s how to set up and use Google Colab for learning bioinformatics with Python:\n\n2.4.1 Step-by-Step Guide to Setting Up Google Colab\n\n2.4.1.1 Access Google Colab\n\nGo to Google Colab in your web browser.\nSign in with your Google account.\n\n\n\n2.4.1.2 Create a New Notebook\n\nClick on File -&gt; New Notebook to create a new notebook.\n\n\n\n2.4.1.3 Install Required Libraries\nGoogle Colab comes with many libraries pre-installed, but you might need to install some additional ones, such as biopython and scikit-bio. You can do this using the !pip install command directly in a cell.\n\n!pip install biopython scikit-bio\n\n\n\n2.4.1.4 Import Libraries and Verify Installation\nIn a new code cell, import the libraries to ensure they are installed correctly.\n\n# Importing necessary libraries\nimport Bio\nimport skbio\n\nprint(\"Biopython version:\", Bio.__version__)\nprint(\"scikit-bio version:\", skbio.__version__)\n\nBiopython version: 1.83\nscikit-bio version: 0.6.0\n\n\n\n\n2.4.1.5 Upload Files to Colab\n\n구글 드라이브에 2024-cnu-lecture 폴더를 만들기\n다운로드 하거나 공유된 (ipynb) 파일을 2024-cnu-lecture 폴더에 복사후 colab으로 open\n분석을 위한 ganbank 파일 ncbi에서 다운로드\n구글 드라이브의 위 디렉토리에 genbank 파일 복사 또는 다음 코드로 로컬 파일을 업로드 가능\n현재 작업 디렉토리 확인\n\n\n!pwd\n\n\n현재 작업 디렉토리를 위 생성한 디렉토리로 변경\n\n\nimport os\nos.chdir('drive/MyDrive/2024-cnu-lecture')\n\n\n!pwd\n\n\n분석을 위한 genbank 등의 파일을 ncbi에서 다운로드 후 위 폴더에 복사\n또는 아래 코드를 이용해서 현재 작업 디렉토리에 업로드\n\n\nfrom google.colab import files\n\nuploaded = files.upload()\n\n# Listing the uploaded files\nfor filename in uploaded.keys():\n    print(filename)\n\n\n\n\nimage.png\n\n\n\n\n2.4.1.6 Example: Reading a GenBank File and Analyzing Sequences\n\nHere’s an example workflow that reads a GenBank file, performs sequence analysis with Biopython, and aligns sequences with scikit-bio:\n위에서 다운로드 받은 genbank 파일을 egfp.gb 이름으로 변경 후 업로드\n아래 예시 코드는 단백질 서열을 읽고 비교하는 코드임\n\n\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom skbio import Protein\nfrom skbio.alignment import global_pairwise_align_protein\n\n# Assuming the uploaded GenBank file is named 'example.gb'\ngenbank_file = 'data/egfp.gb'\n\n# Read a protein sequence from a GenBank file\nrecord = SeqIO.read(genbank_file, \"genbank\")\nprint(len(record.seq))\n\nprotein_seq = record.seq.translate()\nprint(protein_seq)\n\n# Perform alignment with another protein sequence\nseq1 = Protein(str(protein_seq))\nseq2 = Protein(\"MKVLYNLKDG\")\n\nalignment, score, start_end_positions = global_pairwise_align_protein(seq1, seq2)\n\nprint(\"Alignment:\\n\", alignment)\nprint(\"Score:\", score)\nprint(\"Start-End Positions:\", start_end_positions)\n\n1150\nMSRVSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQEVDLCILLS*EVELFPFCVLQLQMEIHRDTRGFCLVLSFVLVVN*KFIHLQEINSLLFKSKGLTYVSVKHWIPKNSKMR*GIFNLVWTSNRLG*PPVKLQQDCLLTQCLRSDVQ*LVLFHFALDFLFANELCF*WFSLFWCVENHLLQGRRQLQDPRRGEVRGRHPGEPHRAEGHRLQGGRQHPGAQAGVQLQQPQRLYHGRQAEERHQGELQDPPQHRGRQRAARRPLPAEHPHRRRPRAAARQPLPEHPVRPEQRPQREARSHGPAGVRDRRRDHSRHGRAVQV\nAlignment:\n TabularMSA[Protein]\n-----------------------------------------------------------------------\nStats:\n    sequence count: 2\n    position count: 384\n-----------------------------------------------------------------------\nMSRVSKGEELFTGVVPILVEL-DGDVNGHKFSV ... PEQRPQREARSHGPAGVRDRRRDHSRHGRAVQV\n--------------MKVLYNLKDG--------- ... ---------------------------------\nScore: 18.0\nStart-End Positions: [(0, 382), (0, 9)]\n\n\n/home/haseong/anaconda3/envs/biopy/lib/python3.11/site-packages/Bio/Seq.py:2880: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n  warnings.warn(\n/home/haseong/anaconda3/envs/biopy/lib/python3.11/site-packages/skbio/alignment/_pairwise.py:538: EfficiencyWarning: You're using skbio's python implementation of Needleman-Wunsch alignment. This is known to be very slow (e.g., thousands of times slower than a native C implementation). We'll be adding a faster version soon (see https://github.com/scikit-bio/scikit-bio/issues/254 to track progress on this).\n  warn(\n\n\n\n\n2.4.1.7 Saving Your Work\nYou can save your notebook to Google Drive:\n\nClick on File -&gt; Save a copy in Drive.\n\n\n\n2.4.1.8 Sharing Your Notebook\nYou can share your notebook with others by clicking on the Share button in the top right corner and setting the appropriate sharing permissions.\n\n\n2.4.1.9 Example Colab Notebook\nHere’s a more comprehensive example notebook that includes reading files, sequence analysis, and visualization: (다음 코드 참고)\nThis notebook covers: - Installing and importing bioinformatics libraries. - Uploading and reading files. - Performing sequence analysis. - Visualizing data.\nBy following these steps and examples, you’ll be well-prepared to use Google Colab for your bioinformatics learning and research.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Example data for visualization\ndata = {\n    'Sequence': ['Seq1', 'Seq2', 'Seq3'],\n    'GC_Content': [50, 55, 45],\n    'Length': [20, 22, 18]\n}\n\n# Visualize using Seaborn\ndf = pd.DataFrame(data)\nsns.histplot(df['GC_Content'], kde=True)\nplt.title('GC Content Distribution')\nplt.xlabel('GC Content (%)')\nplt.show()\n\nsns.boxplot(x=df['Length'])\nplt.title('Sequence Length Distribution')\nplt.xlabel('Length (bp)')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html",
    "href": "cnu_basic.html",
    "title": "3  Python Basics",
    "section": "",
    "text": "3.1 구글 드라이브 연결\n!pwd\nfrom google.colab import drive\nimport os\n\ndrive.mount('/content/drive')\nos.chdir('drive/MyDrive/2024-cnu-lecture')\n!pwd",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#구글-드라이브-연결",
    "href": "cnu_basic.html#구글-드라이브-연결",
    "title": "3  Python Basics",
    "section": "",
    "text": "세션이 다시 시작 될 때에도 실행 필요",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#user",
    "href": "cnu_basic.html#user",
    "title": "3  Python Basics",
    "section": "3.2 [User]",
    "text": "3.2 [User]\n Okay, I’ll learn “1. Basic Python Programming”. Can you teach me with a brief explanation and examples",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#chatgpt",
    "href": "cnu_basic.html#chatgpt",
    "title": "3  Python Basics",
    "section": "3.3 [ChatGPT]",
    "text": "3.3 [ChatGPT]\nAbsolutely! Let’s start with the basics of Python programming. I’ll cover fundamental concepts with brief explanations and examples to get you started.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#python-syntax-and-semantics",
    "href": "cnu_basic.html#python-syntax-and-semantics",
    "title": "3  Python Basics",
    "section": "3.4 Python Syntax and Semantics",
    "text": "3.4 Python Syntax and Semantics\n\n3.4.1 Hello World\nThe simplest Python program:\n\nprint(\"Hello, World!\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#variables-and-data-types",
    "href": "cnu_basic.html#variables-and-data-types",
    "title": "3  Python Basics",
    "section": "3.5 Variables and Data Types",
    "text": "3.5 Variables and Data Types\nPython supports various data types including integers, floats, strings, and booleans.\n\n# Integer\na = 10\n\n# Float\nb = 3.14\n\n# String\nc = \"Hello, Python\"\n\n# Boolean\nd = True\n\nprint(a, b, c, d)\n\n10 3.14 Hello, Python True",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#control-structures",
    "href": "cnu_basic.html#control-structures",
    "title": "3  Python Basics",
    "section": "3.6 Control Structures",
    "text": "3.6 Control Structures\n\n3.6.1 If-Else Statements\nUsed for decision making.\n\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is 5 or less\")\n\nx is greater than 5\n\n\n\n\n3.6.2 Loops\nFor repeating tasks.\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\ni = 0\nwhile i &lt; 5:\n    print(i)\n    i += 1\n\n0\n1\n2\n3\n4\n\n\n\n\n3.6.3 Functions\n\n3.6.3.1 Defining Functions\nReusable blocks of code.\n\ndef greet(name):\n    return f\"Hello, {name}\"\n\nprint(greet(\"Alice\"))\n\nHello, Alice\n\n\n\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\n\nfor name in names:\n  print (\"My name is \", name)\n\nMy name is  Alice\nMy name is  Bob\nMy name is  Charlie\n\n\n\n리스트 값 평균 리턴하는 함수\n\n\ndef average(input):\n    if len(input) == 0:\n        return None\n    return sum(input) / len(input)\n\nx = [1,2,3,4,5,6,7,8,9,10]\nprint(average(x))\n\n5.5\n\n\n\n함수에 대한 개념 이해 중요\n실습과 예제를 통해 이해 필요 (마지막 예제에서 실습)\n\n\n\n3.6.3.2 f-string\n\nf와 {} 조합으로 표현할 수 있으며 출력이 고정된 문자열 맨 앞에 f를 붙여주고, 출력하고 싶은 변수값 등은 중괄호로 표현\n\n\nname = \"Alice\"\nmystr = f\"My name is {name}\"\n\nprint(mystr)\n\nMy name is Alice\n\n\n\nnames = [\"Alice\", \"Alisa\", \"Alister\"]\nmystr = f\"My name is {names}\"\n\nprint(mystr)\n\nMy name is ['Alice', 'Alisa', 'Alister']\n\n\n\nnames = [\"Alice\", \"Alisa\", \"Alister\"]\n\nfor name in names:\n  mystr = f\"My name is {name}\"\n  print(mystr)\n\nMy name is Alice\nMy name is Alisa\nMy name is Alister",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#lists-and-dictionaries",
    "href": "cnu_basic.html#lists-and-dictionaries",
    "title": "3  Python Basics",
    "section": "3.7 Lists and Dictionaries",
    "text": "3.7 Lists and Dictionaries\n\n리스트나 딕셔너리는 파이썬에서 데이터를 저장하는 변수의 자료형임\n여러 종류의 데이터를 효율적으로 활용하기 위한 자료 구조임\n\n\n3.7.1 Lists (리스트)\n\nOrdered, mutable collections.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits[0])  # Accessing elements\nfruits.append(\"date\")  # Adding an element\nprint(fruits)\n\napple\n['apple', 'banana', 'cherry', 'date']\n\n\n\n인덱싱은 값 자체 (1은 두 번째값)\n슬라이싱은 값 사이 경계선 (1은 첫 번째 값과 두 번째 값 사이)\n아래 그림과 여러 실습 예제를 통한 이해 필요\n\n\n\n\nindexing\n\n\n\ngeneids = [x for x in range(10)] # 리스트 컴프리헨션\nprint(geneids)\nprint(geneids[0])\nprint(geneids[-1])\nprint(geneids[2:5])\nprint(geneids[2:-3])\nprint(geneids[:])\nprint(geneids[:-1])\nprint(geneids[1:])\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n0\n9\n[2, 3, 4]\n[2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n리스트 데이터 삽입 삭제\n\n\ngeneids = [1, 2, 3]\nprint(geneids)\ngeneids.append(4)\nprint(geneids)\nprint(\"length: %d\" % len(geneids))\ngeneids[len(geneids):] = [5]\nprint(geneids)\nprint(geneids.pop())\nprint(geneids)\n\n[1, 2, 3]\n[1, 2, 3, 4]\nlength: 4\n[1, 2, 3, 4, 5]\n5\n[1, 2, 3, 4]\n\n\n\n\n3.7.2 Tuple (튜플)\n\n리스트와 같은 기능이지만 ‘(’, ’)’를 사용하고 원소를 변경할 수 없음\n\n리스트보다 빠른 속도, 리스트와 동일한 인덱싱 방법\n\n\ngeneids = (1, 2, 3)\nprint(geneids[0:2])\n#geneids[0] = 4 ## error\n\n(1, 2)\n\n\n\n반복문에서 리스트 또는 튜플 활용\n\n\ngeneids = ['123', '456', '789']\nfor geneid in geneids:\n  print(f\"geneid: {geneid}\")\n\ngeneid: 123\ngeneid: 456\ngeneid: 789\n\n\n\n\n3.7.3 Dictionaries (딕셔너리)\n\nKey-value pairs, unordered.\n\n\nperson = {\"name\": \"Alice\", \"age\": 25}\nprint(person[\"name\"])\nperson[\"age\"] = 26  # Updating value\nprint(person)\n\nAlice\n{'name': 'Alice', 'age': 26}\n\n\n\n키(key)와 값(value)을 쌍으로 저장, ‘{’와’}’를 사용\n\n\ngene_expr = {}\ngene_expr['A'] = 0.5\nprint(gene_expr)\ngene_expr['B'] = 1.2\nprint(gene_expr)\nprint(len(gene_expr))\n\n{'A': 0.5}\n{'A': 0.5, 'B': 1.2}\n2\n\n\n\n인덱싱은 ‘[’, ’]’ 사용, 키 값으로 인덱싱, 정수값 인덱싱 불가\n\n\nprint(gene_expr['A'])\n## gene_expr[0] # error\n\n0.5\n\n\n\n데이터 추가는 key값 value값으로 수행, 삭제는 del 함수 이용\n\n\ngene_expr['C'] = 0.3\nprint(gene_expr)\ndel gene_expr['C']\nprint(gene_expr)\n\n{'A': 0.5, 'B': 1.2, 'C': 0.3}\n{'A': 0.5, 'B': 1.2}\n\n\n\nkey 값과 value 값 구하기\n\n\ngene_expr_keys = list(gene_expr.keys())\nprint(\"keys:\", gene_expr_keys)\ngene_expr_values = list(gene_expr.values())\nprint(\"values:\", gene_expr_values)\n\nkeys: ['A', 'B']\nvalues: [0.5, 1.2]\n\n\n\nin 활용 키 값 탐색\n\n\nprint('D' in gene_expr_keys)\nprint('D' in gene_expr)\nprint('A' in gene_expr)\n\nFalse\nFalse\nTrue\n\n\n\n반복문에서 딕셔너리 활용 items()\n\n\ngene_expr = {'A':0.5, 'B':1.2, 'C':0.3, 'D':3.2}\nfor key, val in gene_expr.items():\n  print(f\"{key} expression value is {val}\")\n  #print(\"%s expression value is %s\" %(geneid, expval))\n\nA expression value is 0.5\nB expression value is 1.2\nC expression value is 0.3\nD expression value is 3.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#modules-and-packages",
    "href": "cnu_basic.html#modules-and-packages",
    "title": "3  Python Basics",
    "section": "3.8 Modules and Packages",
    "text": "3.8 Modules and Packages\n\n3.8.1 Importing Modules\nUsing standard libraries.\n\nimport math\n\nprint(math.sqrt(16))\n\n4.0\n\n\n\n\n3.8.2 Importing Specific Functions\n\nfrom math import sqrt\n\nprint(sqrt(25))\n\n5.0\n\n\n\n위 average 함수를 mystat.py 라는 이름의 파일로 저장, 모듈로 활용\n\n\n#import mystat\n#x = list(range(10))\n#print(mystat.average(x))\n\n\n모듈 직접 실행시 모듈 내 test 코드 실행 (name == main, True)\n\n\n#%run mystat\n\n\n모듈 임포트\n\n\nimport os\nos.getcwd()\n\n'/home/haseong/lecture/cnu-deeplearning-2024'\n\n\n\nfrom os import getcwd\ngetcwd()\n\n'/home/haseong/lecture/cnu-deeplearning-2024'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#file-io",
    "href": "cnu_basic.html#file-io",
    "title": "3  Python Basics",
    "section": "3.9 File I/O",
    "text": "3.9 File I/O\n\n3.9.1 Writing to a File\n\nwith open('example.txt', 'w') as file:\n    file.write(\"Hello, World!\")\n\n\n\n3.9.2 Reading from a File\n\n\nwith open('example.txt', 'r') as file:\n    content = file.read()\n    print(content)\n\n\n\n3.9.3 Example: Basic DNA Sequence Manipulation\nLet’s put some of these concepts together with a simple example that manipulates a DNA sequence.\n\n# Define a DNA sequence\ndna_seq = \"ATGCGTACGTAGCTAGCTAG\"\n\n# Function to compute GC content\ndef gc_content(seq):\n    gc_count = seq.count('G') + seq.count('C')\n    return gc_count / len(seq) * 100\n\n# Function to get the reverse complement of the sequence\ndef reverse_complement(seq):\n    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    return ''.join(complement[base] for base in reversed(seq))\n\n# Compute GC content\ngc = gc_content(dna_seq)\nprint(f\"GC Content: {gc:.2f}%\")\n\n# Get reverse complement\nrev_comp = reverse_complement(dna_seq)\nprint(f\"Reverse Complement: {rev_comp}\")\n\nGC Content: 50.00%\nReverse Complement: CTAGCTAGCTACGTACGCAT\n\n\n\n\n3.9.4 String join (문자열 붙이기)\n\nseq1 = \"ATGC\"\nseq2 = \"TAGC\"\ndna_seq = seq1 + seq2\n\nfor base in dna_seq:\n  print(base)\n\nprint(type(dna_seq))\n\nA\nT\nG\nC\nT\nA\nG\nC\n&lt;class 'str'&gt;\n\n\n\ncomplement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n\nfor base in dna_seq:\n  print(complement [base])\n\ndna_seq_list = []\nfor base in dna_seq:\n  dna_seq_list.append(complement[base])\n\nprint(dna_seq_list)\nprint(type(dna_seq_list))\n\ndna_seq_comp = \"\".join (dna_seq_list)\nprint(dna_seq_comp)\nprint(type(dna_seq_comp))\n\nT\nA\nC\nG\nA\nT\nC\nG\n['T', 'A', 'C', 'G', 'A', 'T', 'C', 'G']\n&lt;class 'list'&gt;\nTACGATCG\n&lt;class 'str'&gt;\n\n\n\ndna_seq_list = [complement[base] for base in dna_seq]\nprint(dna_seq_list)\n\ndna_seq_comp = \"\".join(dna_seq_list)\nprint(dna_seq_comp)\n\nprint(\"\".join([complement[base] for base in dna_seq]))\nprint(\"\".join([complement[base] for base in dna_seq if base == \"A\" or base == \"T\"]))\n\n['T', 'A', 'C', 'G', 'A', 'T', 'C', 'G']\nTACGATCG\nTACGATCG\nTAAT",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#클래스",
    "href": "cnu_basic.html#클래스",
    "title": "3  Python Basics",
    "section": "3.10 클래스",
    "text": "3.10 클래스\n\nGene, Strain class 생성 연습\nGene attribute: name, chromosomal location, length\n\nStrain attribute (변수): name, length of chromosome\n\nStrain method (함수): compute average length of the genes\n\n\nimport statistics\nclass ORF:\n    def __init__(self, location, length, seq):\n        self.location = location\n        self.length = length\n        self.sequence = seq\n\nclass Strain:\n    def __init__(self, name, chrlength):\n        self.name = name\n        self.chr_length = chrlength\n        self.orfs = []\n    def add_orf(self, location, length, seq):\n        self.orfs.append(ORF(location, length, seq))\n    def orf_length_average(self):\n        return statistics.mean([s.length for s in self.orfs])\n\n\necoli = Strain(\"ecoli\", 5000000)\necoli.add_orf(1, 1000, \"ATG\")\necoli.add_orf(1001, 2000, \"CCT\")\necoli.add_orf(2001, 3000, \"ATC\")\n\n\nprint([g.location for g in ecoli.orfs])\nprint([g.sequence for g in ecoli.orfs])\necoli.orf_length_average()\n\n[1, 1001, 2001]\n['ATG', 'CCT', 'ATC']\n\n\n2000\n\n\n\n상속\n\n\nclass Gene(ORF):\n    def add_protein(self, prot_name, prot_seq):\n        self.prot_name = prot_name\n        self.prot_sequence = prot_seq\n        \n\n\ngene1 = Gene(1, 1000, \"ATG\")\nprint(gene1.location)\ngene1.add_protein(\"myprotein\", \"M\")\nprint(gene1.prot_name)\n\n1\nmyprotein",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_basic.html#example-synonymous-mutant-library-generation",
    "href": "cnu_basic.html#example-synonymous-mutant-library-generation",
    "title": "3  Python Basics",
    "section": "3.11 Example: Synonymous Mutant Library Generation",
    "text": "3.11 Example: Synonymous Mutant Library Generation\n\n3.11.1 [User]\n I want to generate mutant library of a protein. I have a nucleotide genbank file for the target protein. I wan to generate all the mutants by changing synonymous codons for each residue. can you give me the python code?\n\n\n\n3.11.2 [ChatGPT]\nCertainly! To generate a mutant library by changing synonymous codons for each residue of a protein, we’ll need to:\n\nRead the nucleotide sequence from a GenBank file.\nTranslate the nucleotide sequence to a protein sequence.\nIdentify synonymous codons for each amino acid.\nGenerate all possible synonymous mutants for the target protein.\n\nHere is the Python code that accomplishes these tasks:\n\n이 예제는 위 프롬프트로 ChatGPT가 만든 코드입니다\n\n\n3.11.2.1 Install Necessary Libraries\nMake sure you have Biopython installed:\n\n!pip install biopython\n\n\n\n3.11.2.2 Python Script to Generate Synonymous Mutant Library\n\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom itertools import product\n\n# Synonymous codon table\nsynonymous_codons = {\n    'F': ['TTT', 'TTC'],\n    'L': ['TTA', 'TTG', 'CTT', 'CTC', 'CTA', 'CTG'],\n    'I': ['ATT', 'ATC', 'ATA'],\n    'M': ['ATG'],\n    'V': ['GTT', 'GTC', 'GTA', 'GTG'],\n    'S': ['TCT', 'TCC', 'TCA', 'TCG', 'AGT', 'AGC'],\n    'P': ['CCT', 'CCC', 'CCA', 'CCG'],\n    'T': ['ACT', 'ACC', 'ACA', 'ACG'],\n    'A': ['GCT', 'GCC', 'GCA', 'GCG'],\n    'Y': ['TAT', 'TAC'],\n    'H': ['CAT', 'CAC'],\n    'Q': ['CAA', 'CAG'],\n    'N': ['AAT', 'AAC'],\n    'K': ['AAA', 'AAG'],\n    'D': ['GAT', 'GAC'],\n    'E': ['GAA', 'GAG'],\n    'C': ['TGT', 'TGC'],\n    'W': ['TGG'],\n    'R': ['CGT', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'],\n    'G': ['GGT', 'GGC', 'GGA', 'GGG'],\n    '*': ['TAA', 'TAG', 'TGA']\n}\n\n# Function to read the GenBank file and extract the coding sequence\ndef read_cds_from_genbank(file_path):\n    record = SeqIO.read(file_path, \"genbank\")\n    for feature in record.features:\n        if feature.type == \"CDS\":\n            return feature.location.extract(record).seq\n    raise ValueError(\"No CDS found in the GenBank file.\")\n\n# Function to generate synonymous mutants\ndef generate_synonymous_mutants(cds_seq):\n    protein_seq = cds_seq.translate()\n    codon_options = []\n\n    for aa in protein_seq[:5]:\n        codons = synonymous_codons.get(aa, [])\n        if not codons:\n            raise ValueError(f\"Unknown amino acid {aa} encountered.\")\n        codon_options.append(codons)\n\n    # Generate all combinations of synonymous codons\n    all_mutants = list(product(*codon_options))\n    return [''.join(mutant) for mutant in all_mutants]\n\n# Main function\ndef main(genbank_file, output_file):\n    cds_seq = read_cds_from_genbank(genbank_file)\n    mutants = generate_synonymous_mutants(cds_seq)\n\n    # Write the mutants to a FASTA file\n    with open(output_file, 'w') as f:\n        for i, mutant in enumerate(mutants):\n            f.write(f\"&gt;Mutant_{i+1}\\n{mutant}\\n\")\n\n# Example usage\ngenbank_file = 'data/egfp.gb'  # Replace with your GenBank file path\noutput_file = 'mutant_library.fasta'\nmain(genbank_file, output_file)\n\n\n위 코드를 두 개의 파일로 나누어 (함수모음 파일, 실행 파일) 모듈 실습 가능",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python Basics</span>"
    ]
  },
  {
    "objectID": "cnu_data_handling.html",
    "href": "cnu_data_handling.html",
    "title": "4  [User]",
    "section": "",
    "text": "from google.colab import drive\nimport os\n\ndrive.mount('/content/drive')\nos.chdir('drive/MyDrive/2024-cnu-lecture')\n\n\n!pwd\n\n I want to learn “2. Data Handling and Analysis”. can you explain me a brief explanation and examples? \n\n4.0.1 [ChatGPT]\nSure! Data handling and analysis are essential skills in Python, especially for tasks like manipulating datasets, performing statistical analyses, and creating visualizations. We’ll focus on the key libraries used for these purposes: NumPy, Pandas, and Matplotlib/Seaborn.\n\n\n4.0.2 1. NumPy\nNumPy is a powerful library for numerical operations and handling arrays.\n\n4.0.2.1 Basics of NumPy\nInstallation:\n!pip install numpy\n\nimport numpy as np\n\n\n# Creating a 1D array\narr1 = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D array\narr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nprint(arr1)\nprint(arr2)\n\n\n# Element-wise operations\narr3 = arr1 * 2\nprint(arr3)\n\n# Mathematical functions\nprint(np.sqrt(arr1))\n\n[1 2 3 4 5]\n[[1 2 3]\n [4 5 6]]\n[ 2  4  6  8 10]\n[1.         1.41421356 1.73205081 2.         2.23606798]\n\n\n\n\n4.0.2.2 Numpy 자료구조 ndarray\n\n행렬이나 다차원 배열 처리용 파이썬 라이브러리\n같은 타입의 데이터만 허용\n리스트에 비해 20배 이상 빠른 속도\nnumpy 배열의 가시화는 일반적으로 우리가 사용하는 방식과 다름\n일반적으로 우리는 차원을 높여가며 그림을 그릴 경우 아래와 같음\n\n\n\n\nalt text\n\n\n\n그런데 numpy에서 차원의 추가는 shape의 앞쪽에 새로운 차원이 추가가 되는 형식\n그림으로 그릴 경우 아래와 같이 axis 0, 1, 2 순서로 해석될 수 있음\n\n\nimport numpy as np\n\ndisplay(np.ones(4))\ndisplay(np.ones((3, 4)))\ndisplay(np.ones((2, 3, 4)))\n\narray([1., 1., 1., 1.])\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]])\n\n\n\n\n\nalt text\n\n\n\n위와 같이 표현되는 것이 맞으나 기존 우리가 알고 있는 가시화 방식과 차이가 있어 혼란 가능\n또한, 아래 pytorch의 tensor dimension 표현법은 또 다른 방식임\n따라서 각 dimension의 axis를 구분하는 것 보다는 depth (channel), width, height를 명확하게 구분하는 것이 중요하고 상황에 따라 다르게 가시화 할 수 있도록 훈련 필요\n\n\n\n\nalt text\n\n\n\nnumpy 객체 만들기\n\n\nimport numpy as np\n\narr = [1, 2, 3]\nprint(arr)\nprint(type(arr))\n\na = np.array([1,2,3])\nprint(a)\nprint(a.dtype)\nprint(a.shape)\nprint(type(a))\n\n[1, 2, 3]\n&lt;class 'list'&gt;\n[1 2 3]\nint64\n(3,)\n&lt;class 'numpy.ndarray'&gt;\n\n\n\narr2 = np.array([[1,2,3], [4,5,6]])\nprint(arr2)\nprint(type(arr2))\nprint(arr2.shape)\nprint(arr2.dtype)\n\n[[1 2 3]\n [4 5 6]]\n&lt;class 'numpy.ndarray'&gt;\n(2, 3)\nint64\n\n\n\nnumpy 자료형\n\n부호가 있는 정수 int(8, 16, 32, 64)\n부호가 없는 정수 uint(8 ,16, 32, 54)\n실수 float(16, 32, 64, 128)\n복소수 complex(64, 128, 256)\n불리언 bool\n문자열 string_\n파이썬 오프젝트 object\n유니코드 unicode_\n\nnp.zeros(), np.ones(), np.arange()\n행렬 연산 지원\n\n\na = np.arange(1, 10).reshape(3,3) # [1, 10)\nprint(a)\na = np.ones((3,4), dtype=np.int16)\nb = np.ones((3,4), dtype=np.int16)\nprint(a)\nprint(b)\nprint(a+b)\nprint(a-b)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]]\n[[2 2 2 2]\n [2 2 2 2]\n [2 2 2 2]]\n[[0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]]\n\n\n\nnumpy 함수\n\nnp.sqrt()\nnp.log()\nnp.square()\nnp.log()\nnp.ceil()\nnp.floor()\nnp.isnan()\nnp.sum()\nnp.mean()\nnp.std()\nnp.min()\n\n\n\ndir(np)\n\n\n\n\n4.0.3 2. Pandas\nPandas is a powerful library for data manipulation and analysis. It provides data structures like DataFrame, which is similar to tables in databases or spreadsheets.\n\n4.0.3.1 Installation\npip install pandas\n\nPandas의 Series는 1차원, DataFrame은 2차원 데이터를 다루는 자료구조\nDataFrames are the primary data structure in pandas, representing tabular data with rows and columns.\nindex, column, values 로 이루어짐\n리스트와 딕셔너리로부터 생성 가능\n숫자형, 문자형, 범주형 등의 다양한 데이터 입력 가능\n\n\n\n\nalt text\n\n\n\nfrom pandas import Series, DataFrame\n\n\ngenes = Series([0.1, 0.2, 1.4, 0.6, 1.1])\nprint(genes)\n\n0    0.1\n1    0.2\n2    1.4\n3    0.6\n4    1.1\ndtype: float64\n\n\n\ngenes = Series([0.1, 0.2, 1.4, 0.6, 1.1], index=['A', 'B', 'C', 'D', 'E'])\nprint(genes)\n\nA    0.1\nB    0.2\nC    1.4\nD    0.6\nE    1.1\ndtype: float64\n\n\n\n\n4.0.3.2 Make a DataFrame\n\nCreate from a list\n\n\nimport pandas as pd\n\ndf = pd.DataFrame([[0, 4, 5], [0, 6, 7], [20, 30, 40]],\n                  index=[1, 2, 3], columns=['P', 'Q', 'R'])\n\nprint(df)\n\n    P   Q   R\n1   0   4   5\n2   0   6   7\n3  20  30  40\n\n\n\nCreate dataframe from a dictionary\n\n\n\n\nalt text\n\n\n\nimport pandas as pd\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Height': [5.5, 6.0, 5.8]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n      Name  Age  Height\n0    Alice   25     5.5\n1      Bob   30     6.0\n2  Charlie   35     5.8\n\n\n\nCreate from a list of dictionary\n\n\n\n\nalt text\n\n\n\ndata = [\n    {'Name': 'Alice', 'Age': 25, 'Height': 5.5},\n    {'Name': 'Bob', 'Age': 30, 'Height': 6.0},\n    {'Name': 'Charlie', 'Age': 35, 'Height': 5.8}\n]\n\ndf = pd.DataFrame(data)\nprint(df)\n\n      Name  Age  Height\n0    Alice   25     5.5\n1      Bob   30     6.0\n2  Charlie   35     5.8\n\n\n\nCreate from Series\n\n\ngenes = {'A': [0.5, 0.1, 0.3],\n         'B': [0.8, 0.9, 0.4]}\nprint(genes)\ngenes_df = DataFrame(genes)\nprint(genes_df)\nprint(genes_df['A'])\nprint(type(genes_df['A']))\n\n\ngenes_df = DataFrame(genes, columns=['B', 'A'], index=['day1', 'day2', 'day3'])\nprint(genes_df)\n\n{'A': [0.5, 0.1, 0.3], 'B': [0.8, 0.9, 0.4]}\n     A    B\n0  0.5  0.8\n1  0.1  0.9\n2  0.3  0.4\n0    0.5\n1    0.1\n2    0.3\nName: A, dtype: float64\n&lt;class 'pandas.core.series.Series'&gt;\n        B    A\nday1  0.8  0.5\nday2  0.9  0.1\nday3  0.4  0.3\n\n\n\n\n4.0.3.3 Dataframe operations\n\ndata = [\n    {'Name': 'Alice', 'Age': 25, 'Height': 5.5},\n    {'Name': 'Bob', 'Age': 30, 'Height': 6.0},\n    {'Name': 'Charlie', 'Age': 35, 'Height': 5.8}\n]\n\ndf = pd.DataFrame(data)\nprint(df)\n\n\n      Name  Age  Height\n0    Alice   25     5.5\n1      Bob   30     6.0\n2  Charlie   35     5.8\n\n\n\n# Selecting columns\nages = df['Age']\nprint(ages)\n\n# Filtering rows\nadults = df[df['Age'] &gt; 18]\nprint(adults)\n\n# Adding a new column\ndf['Weight'] = [65, 70, 75]\nprint(df)\n\n# Grouping and Aggregating\ngrouped = df.groupby('Name').mean()\nprint(grouped)\n\n0    25\n1    30\n2    35\nName: Age, dtype: int64\n      Name  Age  Height\n0    Alice   25     5.5\n1      Bob   30     6.0\n2  Charlie   35     5.8\n      Name  Age  Height  Weight\n0    Alice   25     5.5      65\n1      Bob   30     6.0      70\n2  Charlie   35     5.8      75\n          Age  Height  Weight\nName                         \nAlice    25.0     5.5    65.0\nBob      30.0     6.0    70.0\nCharlie  35.0     5.8    75.0\n\n\n\n인덱스 자동 정렬, 행렬 연산\n\n\ngenes1 = Series([0.1, 0.2, 1.4, 0.6, 1.1], index=['A', 'B', 'C', 'D', 'E'])\ngenes2 = Series([0.1, 0.2, 1.4, 0.6, 1.1], index=['B', 'C', 'D', 'E', 'A'])\ngenes1 + genes2\n\nA    1.2\nB    0.3\nC    1.6\nD    2.0\nE    1.7\ndtype: float64\n\n\n\nprint(genes2.sort_values())\nprint(genes2.sort_index())\n\nB    0.1\nC    0.2\nE    0.6\nA    1.1\nD    1.4\ndtype: float64\nA    1.1\nB    0.1\nC    0.2\nD    1.4\nE    0.6\ndtype: float64\n\n\n\ngenes = {'A': [0.5, 0.1, 0.3],\n         'B': [0.8, 0.9, 0.4]}\n\ngenes_df = DataFrame(genes, columns=['B', 'A'], index=['day1', 'day2', 'day3'])\nprint(genes)\nprint(genes_df)\n\n{'A': [0.5, 0.1, 0.3], 'B': [0.8, 0.9, 0.4]}\n        B    A\nday1  0.8  0.5\nday2  0.9  0.1\nday3  0.4  0.3\n\n\n\nprint(genes_df['A'])\nprint(genes_df.loc['day1'])\nprint(genes_df.index)\nprint(list(genes_df.columns))\n\nday1    0.5\nday2    0.1\nday3    0.3\nName: A, dtype: float64\nB    0.8\nA    0.5\nName: day1, dtype: float64\nIndex(['day1', 'day2', 'day3'], dtype='object')\n['B', 'A']\n\n\n\n\n\n4.0.4 3. Matplotlib/Seaborn\nMatplotlib is a plotting library for creating static, animated, and interactive visualizations. Seaborn is built on top of Matplotlib and provides a high-level interface for drawing attractive statistical graphics.\nInstallation:\npip install matplotlib seaborn\n\nimport matplotlib.pyplot as plt\n\n# Line plot\nplt.plot([1, 2, 3, 4], [1, 4, 9, 16])\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Line Plot')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = [\n    {'Name': 'Alice', 'Age': 25, 'Height': 5.5},\n    {'Name': 'Bob', 'Age': 30, 'Height': 6.0},\n    {'Name': 'Charlie', 'Age': 35, 'Height': 5.8}\n]\n\ndf = pd.DataFrame(data)\nprint(df)\n\n# Scatter plot with Seaborn\nsns.scatterplot(x='Age', y='Height', data=df)\nplt.title('Age vs Height')\nplt.show()\n\n      Name  Age  Height\n0    Alice   25     5.5\n1      Bob   30     6.0\n2  Charlie   35     5.8\n\n\n\n\n\n\n\n\n\n\ndir(sns)\n\n\n\n4.0.5 Example: Analyzing and Visualizing DNA Sequence Data\n\n\n\nalt text\n\n\n\n\n\nalt text\n\n\n\n4.0.5.1 [User]\n I have downloaded a fasta file containing 800 esterase nucleotide sequences. the file is saved at “data/esterase.fa”. can you provide a python code that can read the sequences and compute their length and GC contents ratio? \n\n\n4.0.5.2 [ChatGPT]\nCertainly! To read a FASTA file containing 800 esterase nucleotide sequences and compute their lengths and GC content ratios, you can use the Biopython library. Here is a Python script that accomplishes this:\nInstallation\nFirst, make sure you have Biopython installed:\npip install biopython\n\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\n\n# Function to compute sequence length and GC content\ndef compute_sequence_stats(seq):\n    length = len(seq)\n    gc_content = gc_fraction(seq) * 100  # Convert to percentage\n    return length, gc_content\n\n# Read the FASTA file and process each sequence\ndef process_fasta_file(file_path):\n    records = SeqIO.parse(file_path, \"fasta\")\n    esterase_stats = []\n\n    for record in records:\n        length, gc_content = compute_sequence_stats(record.seq)\n        esterase_stats.append({\n            \"ID\": record.id,\n            \"Length\": length,\n            \"GC_Content\": gc_content\n        })\n\n    return esterase_stats\n\n# Example usage\nfasta_file = 'data/esterases.fasta'  # Replace with your FASTA file path\nesterase_stats = process_fasta_file(fasta_file)\n\n# Print the results\nfor esterase in esterase_stats:\n    print(f\"ID: {esterase['ID']}, Length: {esterase['Length']} bp, GC Content: {esterase['GC_Content']:.2f}%\")\n\n\n\n4.0.5.3 [User]\n I want to store the result above in a file with json csv format. Than I need a code to read the file and convert the data in a DataFrame format. \n\n\n4.0.5.4 [ChatGPT]\nSure! Let’s modify the script to store the results in both JSON and CSV formats. Then, we’ll write a separate script to read these files and convert the data into a Pandas DataFrame.\n\nimport json\nimport csv\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\n\n# Function to compute sequence length and GC content\ndef compute_sequence_stats(seq):\n    length = len(seq)\n    gc_content = gc_fraction(seq) * 100  # Convert to percentage\n    return length, gc_content\n\n# Read the FASTA file and process each sequence\ndef process_fasta_file(file_path):\n    records = SeqIO.parse(file_path, \"fasta\")\n    esterase_stats = []\n\n    for record in records:\n        length, gc_content = compute_sequence_stats(record.seq)\n        esterase_stats.append({\n            \"ID\": record.id,\n            \"Length\": length,\n            \"GC_Content\": gc_content\n        })\n\n    return esterase_stats\n\n# Save results to JSON and CSV\ndef save_results(esterase_stats, json_file, csv_file):\n    # Save to JSON\n    with open(json_file, 'w') as jf:\n        json.dump(esterase_stats, jf, indent=4)\n\n    # Save to CSV\n    with open(csv_file, 'w', newline='') as cf:\n        writer = csv.DictWriter(cf, fieldnames=['ID', 'Length', 'GC_Content'])\n        writer.writeheader()\n        for esterase in esterase_stats:\n            writer.writerow(esterase)\n\n# Example usage\nfasta_file = 'data/esterases.fasta'  # Replace with your FASTA file path\njson_file = 'data/esterase_stats.json'\ncsv_file = 'data/esterase_stats.csv'\n\nesterase_stats = process_fasta_file(fasta_file)\nsave_results(esterase_stats, json_file, csv_file)\n\nprint(f\"Results saved to {json_file} and {csv_file}\")\n\nResults saved to data/esterase_stats.json and data/esterase_stats.csv\n\n\n\nimport pandas as pd\nimport json\n\n# Load data from JSON file\nwith open('data/esterase_stats.json', 'r') as jf:\n    esterase_stats_json = json.load(jf)\n\n# Convert to DataFrame\ndf_json = pd.DataFrame(esterase_stats_json)\nprint(\"DataFrame from JSON file:\")\nprint(df_json.head())\n\n# Load data from CSV file\ndf_csv = pd.read_csv('data/esterase_stats.csv')\nprint(\"DataFrame from CSV file:\")\nprint(df_csv.head())\n\n\n\n4.0.5.5 [User]\n I want a python code that display the statistics of the GC distribution and boxplot of the length. \n\n\n4.0.5.6 [ChatGPT]\nSure! Let’s create a Python script that reads the data from the previously saved JSON or CSV file, computes statistics for the GC content distribution, and generates a boxplot for the sequence lengths using Pandas and Matplotlib.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data from CSV file\ndf = pd.read_csv('data/esterase_stats.csv')\n\n# Compute statistics for GC content distribution\ngc_stats = df['GC_Content'].describe()\nprint(\"GC Content Distribution Statistics:\")\nprint(gc_stats)\n\n# Generate a histogram for GC content distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df['GC_Content'], bins=30, kde=True)\nplt.title('GC Content Distribution')\nplt.xlabel('GC Content (%)')\nplt.ylabel('Frequency')\nplt.show()\n\n# Generate a boxplot for sequence lengths\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=df['Length'])\nplt.title('Boxplot of Sequence Lengths')\nplt.xlabel('Sequence Length (bp)')\nplt.show()\n\nGC Content Distribution Statistics:\ncount    355.000000\nmean      51.362399\nstd        5.551809\nmin       29.739777\n25%       50.052780\n50%       51.901566\n75%       54.322222\nmax       75.683891\nName: GC_Content, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScript\n\n\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\n\n# Read a FASTA file\nfasta_file = 'data/esterases.fasta'\nresults = SeqIO.parse(fasta_file, 'fasta')\n\nfor record in results:\n    print(record.id)\n    print(record.seq)\n    print(gc_fraction(record.seq))\n\nprint(results)\n\n\nCalculate\n\n\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\n\n# Read a FASTA file\nfasta_file = 'data/esterases.fasta'\nresults = SeqIO.parse(fasta_file, 'fasta')\n\nfor record in results:\n    seq = record.seq\n    length = len(seq)\n    gc_content = gc_fraction(seq) * 100  # Convert to percentage\n    print(f\"ID: {record.id}, Length: {length} bp, GC Content: {gc_content:.2f}%\")\n    \n\n\nstore in a list\n\n\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\nimport pandas as pd\n\n# Read a FASTA file\nfasta_file = 'data/esterases.fasta'\nresults = SeqIO.parse(fasta_file, 'fasta')\nesterase_stats = []\n\nfor record in results:\n    seq = record.seq\n    length = len(seq)\n    gc_content = gc_fraction(seq) * 100  # Convert to percentage\n    esterase_stats.append({\n        \"ID\": record.id,\n        \"Length\": length,\n        \"GC_Content\": gc_content\n    })\n\nprint(esterase_stats) \npd.DataFrame(esterase_stats)\n\n\nmake a function\n\n\nfrom Bio import SeqIO\nfrom Bio.SeqUtils import gc_fraction\nimport pandas as pd\n\n# Read a FASTA file\nfasta_file = 'data/esterases.fasta'\nresults = SeqIO.parse(fasta_file, 'fasta')\nesterase_stats = []\n\n# Function to compute sequence length and GC content\ndef compute_sequence_stats(seq):\n    length = len(seq)\n    gc_content = gc_fraction(seq) * 100  # Convert to percentage\n    return length, gc_content\n\n\nfor record in results:\n    seq = record.seq\n    length, gc_content = compute_sequence_stats(seq)\n    esterase_stats.append({\n        \"ID\": record.id,\n        \"Length\": length,\n        \"GC_Content\": gc_content\n    })\n\nprint(esterase_stats) \npd.DataFrame(esterase_stats)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>[User]</span>"
    ]
  },
  {
    "objectID": "cnu_bioinformatics.html",
    "href": "cnu_bioinformatics.html",
    "title": "5  Bioinformatics",
    "section": "",
    "text": "5.1 [User]\nI want to learn about “3. Bioinformatics Libraries”. Can you explain it with example codes?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bioinformatics</span>"
    ]
  },
  {
    "objectID": "cnu_bioinformatics.html#chatgpt",
    "href": "cnu_bioinformatics.html#chatgpt",
    "title": "5  Bioinformatics",
    "section": "5.2 [ChatGPT]",
    "text": "5.2 [ChatGPT]\nCertainly! Bioinformatics libraries in Python, such as Biopython and scikit-bio, provide tools and functions to handle biological data, including sequence analysis, structure analysis, and more. Here’s a brief introduction to these libraries with example codes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bioinformatics</span>"
    ]
  },
  {
    "objectID": "cnu_bioinformatics.html#biopython",
    "href": "cnu_bioinformatics.html#biopython",
    "title": "5  Bioinformatics",
    "section": "5.3 Biopython",
    "text": "5.3 Biopython\nBiopython is one of the most widely used libraries for bioinformatics in Python. It provides tools for reading and writing different bioinformatics file formats, performing sequence analysis, and interacting with biological databases.\n\nfrom google.colab import drive\nimport os\n\ndrive.mount('/content/drive')\nos.chdir('drive/MyDrive/2024-cnu-lecture')\n\nMounted at /content/drive\n\n\n\n5.3.1 Installation\n\n!pip install biopython\n\n\n\n5.3.2 Sequence objects\n\nfrom Bio.Seq import Seq\nmy_seq = Seq(\"AGTACACTGGT\")\nmy_seq\n\nSeq('AGTACACTGGT')\n\n\n\nfor index, letter in enumerate(my_seq):\n    print(\"%i %s \" % (index, letter))\n\n0 A \n1 G \n2 T \n3 A \n4 C \n5 A \n6 C \n7 T \n8 G \n9 G \n10 T \n\n\n\nx = [1, 4, 5, 7, 8]\nfor i in enumerate(x):\n    print(i)\n\nfor i in x:\n    print(i)\n\n(0, 1)\n(1, 4)\n(2, 5)\n(3, 7)\n(4, 8)\n1\n4\n5\n7\n8\n\n\n\nprint(my_seq)\nprint(my_seq[0:3])\nprint(my_seq[0::2])\nprint(str(my_seq))\nprint(my_seq + \"ATG\")\nprint(my_seq==\"ATG\")\nprint(\"AGT\" in my_seq)\n\nAGTACACTGGT\nAGT\nATCCGT\nAGTACACTGGT\nAGTACACTGGTATG\nFalse\nTrue\n\n\n\nmy_seq_low = my_seq.lower()\nprint(my_seq_low)\nprint(my_seq_low.upper())\nprint(my_seq.complement())\nprint(my_seq.reverse_complement())\n\nagtacactggt\nAGTACACTGGT\nTCATGTGACCA\nACCAGTGTACT\n\n\n\nTranscription, Translation\n\n\nmrna = my_seq.transcribe()\nprint(mrna)\nprot = mrna.translate() ## truncated\nprint(prot)\nprint(my_seq.translate())\n\nAGUACACUGGU\nSTL\nSTL\n\n\n/home/haseong/anaconda3/envs/biopy/lib/python3.11/site-packages/Bio/Seq.py:2880: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n  warnings.warn(\n\n\n\nfrom Bio.Data import CodonTable\nstandard_table = CodonTable.unambiguous_dna_by_id[1]\nprint(standard_table)\nprint(standard_table.start_codons)\nprint(standard_table.stop_codons)\nprint(type(standard_table))\n\nTable 1 Standard, SGC0\n\n  |  T      |  C      |  A      |  G      |\n--+---------+---------+---------+---------+--\nT | TTT F   | TCT S   | TAT Y   | TGT C   | T\nT | TTC F   | TCC S   | TAC Y   | TGC C   | C\nT | TTA L   | TCA S   | TAA Stop| TGA Stop| A\nT | TTG L(s)| TCG S   | TAG Stop| TGG W   | G\n--+---------+---------+---------+---------+--\nC | CTT L   | CCT P   | CAT H   | CGT R   | T\nC | CTC L   | CCC P   | CAC H   | CGC R   | C\nC | CTA L   | CCA P   | CAA Q   | CGA R   | A\nC | CTG L(s)| CCG P   | CAG Q   | CGG R   | G\n--+---------+---------+---------+---------+--\nA | ATT I   | ACT T   | AAT N   | AGT S   | T\nA | ATC I   | ACC T   | AAC N   | AGC S   | C\nA | ATA I   | ACA T   | AAA K   | AGA R   | A\nA | ATG M(s)| ACG T   | AAG K   | AGG R   | G\n--+---------+---------+---------+---------+--\nG | GTT V   | GCT A   | GAT D   | GGT G   | T\nG | GTC V   | GCC A   | GAC D   | GGC G   | C\nG | GTA V   | GCA A   | GAA E   | GGA G   | A\nG | GTG V   | GCG A   | GAG E   | GGG G   | G\n--+---------+---------+---------+---------+--\n['TTG', 'CTG', 'ATG']\n['TAA', 'TAG', 'TGA']\n&lt;class 'Bio.Data.CodonTable.NCBICodonTableDNA'&gt;\n\n\n\nMutableSeq objects\n\n\nfrom Bio.Seq import Seq\nmy_seq = Seq(\"AGTACACTGGT\")\nmy_seq[5] = \"G\"\n\n\nfrom Bio.Seq import MutableSeq\n\nmutable_seq = MutableSeq(my_seq)\nmutable_seq\n\nmutable_seq[5] = \"G\"\nmutable_seq\n\nnew_seq = Seq(mutable_seq)\nnew_seq\n\nSeq('AGTACGCTGGT')\n\n\n\nFinding subsequence\n\n\nseq = Seq(\"GCCATTGTAATGGGCCGCTGAAAGGGTGCCCGA\")\nprint(seq.index(\"ATG\"))\nprint(seq.find(\"ATG\"))\nprint(seq.find(\"AAAAA\"))\n\n\n\n5.3.3 SeqRecord\n\nSequence annotation objects\n특정 서열의 identifier나 feature 정보 포함\n\n\nfrom Bio.Seq import Seq\nfrom Bio.SeqRecord import SeqRecord\n\nsimple_seq = Seq(\"GATC\")\nsimple_seq_r = SeqRecord(simple_seq)\n\n\nhelp(SeqRecord)\n\n\nclass 개념 이해 필요\n\n\nsimple_seq_r.id = \"AC12345\"\nsimple_seq_r.description = \"Made up sequence I wish I could write a paper about\"\nprint(simple_seq_r.description)\nprint(simple_seq_r.seq)\n\n\nRead fasta file\nhttps://www.ncbi.nlm.nih.gov/nuccore/MW015936.1\nZika virus isolate Zika virus/H.sapiens-tc/THA/2006/CVD_06-020, complete genome\n\n\n\n\nalt text\n\n\n\n인용구 추가\n\n\nfrom Bio import SeqIO\nrecord = SeqIO.read(\"data/MW015936.gb\", \"gb\")\n\n\nprint(record)\nhelp(record)\n\n\nprint(record.id)\nprint(record.name)\nprint(record.description)\nprint(record.seq)\nprint(record.features)\n\n\nprint(len(record.seq))\nlist(record.features)\n\n\n\n\nalt text\n\n\n\nSlicing\n\n\nprint(record.features[0])\nprint(record.features[1])\nprint(record.features[2])\n\n\nextract sequence\n\n\nrecord.features[2].extract(record.seq)\n\nSeq('ATGAAAAACCCAAAGAAGAAATCCGGAGGATTCCGGATTGTCAATATGCTAAAA...TAA')\n\n\n\n위치 탐색 in 활용\n\n\nfor feature in record.features:\n    if 4350 in feature:\n        print(f\"type: {feature.type} start: {feature.location.start} end: {feature.location.end}\")\n\ntype: source start: 0 end: 10807\ntype: gene start: 107 end: 10379\ntype: CDS start: 107 end: 10379\ntype: mat_peptide start: 4223 end: 4613\n\n\n\n서열 위치로 직접 슬라이싱\n\n\nsub_record = record[3000:5000]\nprint(sub_record)\nprint(len(sub_record))\nprint(len(sub_record.features))\nprint(sub_record.features[0])\nprint(sub_record.features[1])\n\n\n5.3.3.1 Example - find features and plot\n[User]\n I have a genbank file. I want extract all the features of “mat_peptide” type in the file and calculate their length. can you show me a python code? \n[ChatGPT]\nCertainly! You can use Biopython to read a GenBank file and extract features of a specific type, such as “mat_peptide”. Here’s how you can do it:\nPython Script\nHere is the Python script that extracts all “mat_peptide” features from a GenBank file and calculates their lengths:\nfrom Bio import SeqIO\n\n# Function to extract mat_peptide features and calculate their lengths\ndef extract_mat_peptide_features(file_path):\n    record = SeqIO.read(file_path, \"genbank\")\n    mat_peptides = []\n\n    for feature in record.features:\n        if feature.type == \"mat_peptide\":\n            start = feature.location.start.position\n            end = feature.location.end.position\n            length = end - start\n            mat_peptides.append({\n                \"ID\": feature.qualifiers.get(\"product\", [\"Unknown\"])[0],\n                \"Start\": start,\n                \"End\": end,\n                \"Length\": length\n            })\n    \n    return mat_peptides\n\n# Example usage\ngenbank_file = 'example.gb'  # Replace with your GenBank file path\nmat_peptide_features = extract_mat_peptide_features(genbank_file)\n\n# Print the results\nfor peptide in mat_peptide_features:\n    print(f\"ID: {peptide['ID']}, Start: {peptide['Start']}, End: {peptide['End']}, Length: {peptide['Length']} bp\")\nExplanation\n\nImport Libraries: Import the necessary modules from Biopython.\nextract_mat_peptide_features Function: This function reads the GenBank file, extracts features of type “mat_peptide”, calculates their start and end positions, and determines their lengths.\nExample Usage: Replace 'example.gb' with the path to your GenBank file. The script will read the file, extract the “mat_peptide” features, and print their IDs, start and end positions, and lengths.\n\nRunning the Script\n\nSave the script to a file, for example, extract_mat_peptides.py.\nEnsure you have the GenBank file (e.g., example.gb) in the same directory.\nRun the script from the command line:\npython extract_mat_peptides.py\n\nThis script will read the GenBank file, extract all “mat_peptide” features, calculate their lengths, and print the details. If you have any specific questions or need further assistance, feel free to ask!\n\nFind a specific feature\n\n\nfrom Bio import SeqIO\n\nrecords = SeqIO.parse(\"data/MW015936.gb\", \"genbank\")\nfor feature in record.features:\n    if feature.type == \"mat_peptide\":\n        print(feature.qualifiers[\"product\"])\n        print(feature.location.start, feature.location.end)\n\n\nget length and plot\n\n\nfrom Bio import SeqIO\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nmat_peptides = []\nrecords = SeqIO.parse(\"data/MW015936.gb\", \"genbank\")\nfor feature in record.features:\n    if feature.type == \"mat_peptide\":\n        mat_peptides.append({\n            \"id\": feature.qualifiers[\"product\"],\n            \"start\": feature.location.start,\n            \"end\": feature.location.end,\n            \"length\": feature.location.end - feature.location.start\n        })\n\ndf = pd.DataFrame(mat_peptides)\n\nplt.figure(figsize = (10, 6))\n# df[\"length\"].hist()\nsns.histplot(df[\"length\"])\nplt.xticks(rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3.4 Entrez\n\nProvides code to access NCBI over the WWW\n파일 읽기/쓰기 with 문 사용\n\n\nfrom Bio import Entrez\nEntrez.email = \"kimhaseong@gmail.com\"\n\n# Searching NCBI Nucleotide database\nhandle = Entrez.esearch(db=\"nucleotide\", term=\"Homo sapiens[Organism] AND BRCA1[Gene]\", retmax=5)\nrecord = Entrez.read(handle)\nprint(record[\"IdList\"])\n\n['262359905', '2703623457', '2703623455', '2703623453', '2703623451']\n\n\n\nfrom Bio import Entrez\nfrom Bio import SeqIO\nentid = record[\"IdList\"][0]\n\nwith Entrez.efetch(db=\"nucleotide\", rettype=\"gb\", retmode=\"text\", id=entid) as handle:\n    seq_record = SeqIO.read(handle, \"gb\")\nprint(f\"{seq_record.id} with {len(seq_record.features)} features\")\nprint(type(seq_record))\nSeqIO.write(seq_record, \"/content/drive/My Drive/2024-cnu-lecture/data/\"+seq_record.id+\".fa\", \"fasta\")\n#SeqIO.write(seq_record, seq_record.id+\".fa\", \"fasta\")\nprint(seq_record)\n\n\n여러개 record에 대해서는 parse 함수를 사용\n\n\nfrom Bio import SeqIO\n\nentids = record[\"IdList\"]\n\nwith Entrez.efetch(db=\"nucleotide\", rettype=\"gb\", retmode=\"text\", id=entids) as handle:\n    for seq_record in SeqIO.parse(handle, \"gb\"):\n        print(f\"{seq_record.id} {seq_record.description[:50]}\")\n        print(f\"Sequence length {len(seq_record)}, {len(seq_record.features)} \\\n              features, from {seq_record.annotations['source']}\")\n        print(\"\\n\")\n        SeqIO.write(seq_record, \"/content/drive/My Drive/2024-cnu-lecture/data/\"+seq_record.id+\".gb\", \"gb\")\n\nNG_005905.2 Homo sapiens BRCA1 DNA repair associated (BRCA1), \nSequence length 193689, 130               features, from Homo sapiens (human)\n\n\nPP465840.1 Homo sapiens isolate TWH-3503-0-1 breast and ovari\nSequence length 5592, 3               features, from Homo sapiens (human)\n\n\nPP465839.1 Homo sapiens isolate TWH-3747-0-1 truncated breast\nSequence length 891, 3               features, from Homo sapiens (human)\n\n\nPP465838.1 Homo sapiens isolate TWH-3713-0-1 truncated breast\nSequence length 3462, 3               features, from Homo sapiens (human)\n\n\nPP465837.1 Homo sapiens isolate OV-0673-0-1 truncated breast \nSequence length 4167, 3               features, from Homo sapiens (human)\n\n\n\n\n\n\n5.3.5 Pairwise sequence alignment\n\nfrom Bio import Align\n\naligner = Align.PairwiseAligner()\naligner.mode = \"global\"\naligner.mismatch_score = -10\nalignments = aligner.align(\"AAACAAA\", \"ACGAAAGAAA\")\nprint(aligner)\nprint(len(alignments))\nprint(alignments.score)\n\nprint(alignments[0])\nprint(alignments[1])\nprint(alignments[2])\n\nPairwise sequence aligner with parameters\n  wildcard: None\n  match_score: 1.000000\n  mismatch_score: -10.000000\n  target_internal_open_gap_score: 0.000000\n  target_internal_extend_gap_score: 0.000000\n  target_left_open_gap_score: 0.000000\n  target_left_extend_gap_score: 0.000000\n  target_right_open_gap_score: 0.000000\n  target_right_extend_gap_score: 0.000000\n  query_internal_open_gap_score: 0.000000\n  query_internal_extend_gap_score: 0.000000\n  query_left_open_gap_score: 0.000000\n  query_left_extend_gap_score: 0.000000\n  query_right_open_gap_score: 0.000000\n  query_right_extend_gap_score: 0.000000\n  mode: global\n\n12\n6.0\ntarget            0 A--AACA-AA-  7\n                  0 |--||-|-||- 11\nquery             0 ACGAA-AGAAA 10\n\ntarget            0 A--AACA-A-A  7\n                  0 |--||-|-|-| 11\nquery             0 ACGAA-AGAAA 10\n\ntarget            0 A--AACA--AA  7\n                  0 |--||-|--|| 11\nquery             0 ACGAA-AGAAA 10\n\n\n\n\naligner = Align.PairwiseAligner(mismatch_score=-10, mode=\"local\")\nalignments = aligner.align(\"AAACAAA\", \"ACGAAAGAAA\")\nprint(len(alignments))\nprint(alignments.score)\n\nprint(alignments[0])\nprint(alignments[1])\nprint(alignments[2])\n\n12\n6.0\ntarget            0 A--AACA-AA  7\n                  0 |--||-|-|| 10\nquery             0 ACGAA-AGAA  9\n\ntarget            0 A--AACA-A-A  7\n                  0 |--||-|-|-| 11\nquery             0 ACGAA-AGAAA 10\n\ntarget            0 A--AACA--AA  7\n                  0 |--||-|--|| 11\nquery             0 ACGAA-AGAAA 10\n\n\n\n\n5.3.5.1 Alignment objects\n\n\nprint(alignments[0])\nprint(type(alignments[0]))\nprint(alignments[0].shape)\nprint(alignments[0].indices)\n\nprint(alignments[0].counts())\ndisplay(alignments[0].frequencies)\n\ntarget            0 A--AACA-AA  7\n                  0 |--||-|-|| 10\nquery             0 ACGAA-AGAA  9\n\n&lt;class 'Bio.Align.Alignment'&gt;\n(2, 10)\n[[ 0 -1 -1  1  2  3  4 -1  5  6]\n [ 0  1  2  3  4 -1  5  6  7  8]]\nAlignmentCounts(gaps=4, identities=6, mismatches=0)\n\n\n{'A': array([2., 0., 0., 2., 2., 0., 2., 0., 2., 2.]),\n '-': array([0., 1., 1., 0., 0., 1., 0., 1., 0., 0.]),\n 'C': array([0., 1., 0., 0., 0., 1., 0., 0., 0., 0.]),\n 'G': array([0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])}\n\n\n\n\n5.3.5.2 [User] biopython을 사용해서 NCBI에서 egfp 와 sfgfp 서열을 다운로드 받고 서열을 비교하는 코드를 알려줘\n\n\n5.3.5.3 [ChatGPT]\nfrom Bio import Entrez, SeqIO\nfrom Bio.Seq import Seq\nfrom Bio.SeqRecord import SeqRecord\nfrom Bio import pairwise2\nfrom Bio.pairwise2 import format_alignment\n\n# Always provide your email when using NCBI's Entrez\nEntrez.email = \"your.email@example.com\"  # Replace with your email address\n\ndef download_sequence(accession):\n    \"\"\"Download a sequence by its accession number from NCBI.\"\"\"\n    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n    seq_record = SeqIO.read(handle, \"genbank\")\n    handle.close()\n    return seq_record\n\ndef compare_sequences(seq1, seq2):\n    \"\"\"Perform a simple sequence alignment between two sequences.\"\"\"\n    alignments = pairwise2.align.globalxx(seq1.seq, seq2.seq)\n    return alignments[0]  # Return the first alignment\n\n# Accession numbers for EGFP and sfGFP from NCBI\negfp_accession = \"U55762\"  # Example accession, update as necessary\nsfgfp_accession = \"EU304438\"  # Example accession, update as necessary\n\n# Download sequences\negfp_seq = download_sequence(egfp_accession)\nsfgfp_seq = download_sequence(sfgfp_accession)\n\n# Compare sequences\nalignment = compare_sequences(egfp_seq, sfgfp_seq)\n\n# Print alignment\nprint(format_alignment(*alignment))\n\n\n5.3.5.4 [User] 출력을 좀 더 보기 좋게 해줘\n\n\n5.3.5.5 [ChatGTP]\nfrom Bio import Entrez, SeqIO\nfrom Bio import pairwise2\nfrom Bio.pairwise2 import format_alignment\n\n# Always provide your email when using NCBI's Entrez\nEntrez.email = \"your.email@example.com\"  # Replace with your email address\n\ndef download_sequence(accession):\n    \"\"\"Download a sequence by its accession number from NCBI.\"\"\"\n    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n    seq_record = SeqIO.read(handle, \"genbank\")\n    handle.close()\n    return seq_record\n\ndef compare_sequences(seq1, seq2):\n    \"\"\"Perform a sequence alignment between two sequences and format the output.\"\"\"\n    # Using a simple scoring: 1 point for match, -1 for mismatch, -2 for opening a gap, -0.5 for extending it\n    alignments = pairwise2.align.globalms(seq1.seq, seq2.seq, 2, -1, -2, -0.5, one_alignment_only=True)\n    best_alignment = alignments[0]\n    return best_alignment\n\ndef print_alignment(alignment):\n    \"\"\"Print formatted alignment.\"\"\"\n    aligned_seq1, aligned_seq2, score, start, end = alignment\n    match_line = []  # To store the match symbols ('|', ' ', '*')\n\n    for base1, base2 in zip(aligned_seq1, aligned_seq2):\n        if base1 == base2:\n            match_line.append('|')  # Match\n        else:\n            match_line.append(' ')  # Mismatch\n\n    # Print the results\n    print(\"Alignment:\")\n    print(\"Score:\", score)\n    print(aligned_seq1[start:end])\n    print(''.join(match_line[start:end]))\n    print(aligned_seq2[start:end])\n\n# Accession numbers for EGFP and sfGFP\negfp_accession = \"U55762\"  # Example accession, update as necessary\nsfgfp_accession = \"EU304438\"  # Example accession, update as necessary\n\n# Download sequences\negfp_seq = download_sequence(egfp_accession)\nsfgfp_seq = download_sequence(sfgfp_accession)\n\n# Compare sequences\nalignment = compare_sequences(egfp_seq, sfgfp_seq)\n\n# Print formatted alignment\nprint_alignment(alignment)\n\n\n5.3.5.6 [User] 위 코드의 서열 비교 출력을 지정된 base pair 길이만큼 끊어서 보여주는 코드를 만들어줘\n\n\n5.3.5.7 [ChatGPT]\nfrom Bio import Entrez, SeqIO\nfrom Bio import pairwise2\nfrom Bio.pairwise2 import format_alignment\n\n# Always provide your email when using NCBI's Entrez\nEntrez.email = \"your.email@example.com\"  # Replace with your email address\n\ndef download_sequence(accession):\n    \"\"\"Download a sequence by its accession number from NCBI.\"\"\"\n    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n    seq_record = SeqIO.read(handle, \"genbank\")\n    handle.close()\n    return seq_record\n\ndef compare_sequences(seq1, seq2):\n    \"\"\"Perform a sequence alignment between two sequences and format the output.\"\"\"\n    alignments = pairwise2.align.globalms(seq1.seq, seq2.seq, 2, -1, -2, -0.5, one_alignment_only=True)\n    return alignments[0]  # Return the first alignment\n\ndef print_alignment(alignment, chunk_size=60):\n    \"\"\"Print formatted alignment in chunks.\"\"\"\n    aligned_seq1, aligned_seq2, score, start, end = alignment\n    alignment_length = end - start\n    print(\"Alignment Score:\", score)\n    \n    for i in range(0, alignment_length, chunk_size):\n        subseq1 = aligned_seq1[start+i:start+i+chunk_size]\n        subseq2 = aligned_seq2[start+i:start+i+chunk_size]\n        \n        # Generate a matching line\n        match_line = ''.join('|' if b1 == b2 else ' ' for b1, b2 in zip(subseq1, subseq2))\n        \n        # Print the alignment in chunks\n        print(\"\\nAlignment chunk from position\", start+i+1, \"to\", min(start+i+chunk_size, alignment_length))\n        print(subseq1)\n        print(match_line)\n        print(subseq2)\n\n# Accession numbers for EGFP and sfGFP\negfp_accession = \"U55762\"  # Example accession, update as necessary\nsfgfp_accession = \"EU304438\"  # Example accession, update as necessary\n\n# Download sequences\negfp_seq = download_sequence(egfp_accession)\nsfgfp_seq = download_sequence(sfgfp_accession)\n\n# Compare sequences\nalignment = compare_sequences(egfp_seq, sfgfp_seq)\n\n# Print formatted alignment\nprint_alignment(alignment, chunk_size=60)  # You can adjust the chunk size as needed\n\n\n\n5.3.5.8 [User] 출력 서열 앞쪽에 각 서열의 accession number를 붙여주고 마지막에는 총 길이와 다른 서열의 개수 등 통계를 출력해줘\n\n\n5.3.5.9 [ChatGPT]\nfrom Bio import Entrez, SeqIO\nfrom Bio import pairwise2\nfrom Bio.pairwise2 import format_alignment\n\n# Always provide your email when using NCBI's Entrez\nEntrez.email = \"your.email@example.com\"  # Replace with your email address\n\ndef download_sequence(accession):\n    \"\"\"Download a sequence by its accession number from NCBI.\"\"\"\n    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n    seq_record = SeqIO.read(handle, \"genbank\")\n    handle.close()\n    return seq_record\n\ndef compare_sequences(seq1, seq2):\n    \"\"\"Perform a sequence alignment between two sequences and format the output.\"\"\"\n    alignments = pairwise2.align.globalms(seq1.seq, seq2.seq, 2, -1, -2, -0.5, one_alignment_only=True)\n    return alignments[0]  # Return the first alignment\n\ndef print_alignment(alignment, seq1_acc, seq2_acc, chunk_size=60):\n    \"\"\"Print formatted alignment in chunks, including accession numbers and statistics.\"\"\"\n    aligned_seq1, aligned_seq2, score, start, end = alignment\n    alignment_length = end - start\n    total_mismatches = sum(1 for b1, b2 in zip(aligned_seq1, aligned_seq2) if b1 != b2)\n    \n    print(f\"Alignment Score: {score}\")\n    print(f\"Total Length: {alignment_length}\")\n    print(f\"Mismatches: {total_mismatches}\")\n    \n    for i in range(0, alignment_length, chunk_size):\n        subseq1 = aligned_seq1[start+i:start+i+chunk_size]\n        subseq2 = aligned_seq2[start+i:start+i+chunk_size]\n        \n        # Generate a matching line\n        match_line = ''.join('|' if b1 == b2 else ' ' for b1, b2 in zip(subseq1, subseq2))\n        \n        # Print the alignment in chunks\n        print(f\"\\n{seq1_acc} (position {start+i+1} to {min(start+i+chunk_size, alignment_length)})\")\n        print(subseq1)\n        print(match_line)\n        print(f\"{seq2_acc} (position {start+i+1} to {min(start+i+chunk_size, alignment_length)})\")\n        print(subseq2)\n\n# Accession numbers for EGFP and sfGFP\negfp_accession = \"U55762\"  # Example accession, update as necessary\nsfgfp_accession = \"EU304438\"  # Example accession, update as necessary\n\n# Download sequences\negfp_seq = download_sequence(egfp_accession)\nsfgfp_seq = download_sequence(sfgfp_accession)\n\n# Compare sequences\nalignment = compare_sequences(egfp_seq, sfgfp_seq)\n\n# Print formatted alignment\nprint_alignment(alignment, egfp_accession, sfgfp_accession, chunk_size=60)  # You can adjust the chunk size as needed\n\n!pip install matplotlib\n\n\n\n5.3.5.10 [User] 위 비교 결과를 한 줄로 표현되는 그래프로 그려주는 코드를 알려줘\n\n\n5.3.5.11 [ChatGPT]\nfrom Bio import Entrez, SeqIO\nfrom Bio import pairwise2\nimport matplotlib.pyplot as plt\n\n# Always provide your email when using NCBI's Entrez\nEntrez.email = \"your.email@example.com\"  # Replace with your email address\n\ndef download_sequence(accession):\n    \"\"\"Download a sequence by its accession number from NCBI.\"\"\"\n    handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n    seq_record = SeqIO.read(handle, \"genbank\")\n    handle.close()\n    return seq_record\n\ndef compare_sequences(seq1, seq2):\n    \"\"\"Perform a sequence alignment between two sequences and format the output.\"\"\"\n    alignments = pairwise2.align.globalms(seq1.seq, seq2.seq, 2, -1, -2, -0.5, one_alignment_only=True)\n    return alignments[0]  # Return the first alignment\n\ndef plot_alignment(alignment):\n    \"\"\"Plot alignment as a line graph.\"\"\"\n    aligned_seq1, aligned_seq2, score, start, end = alignment\n    \n    # Create match scores: 1 for match, 0 for mismatch\n    match_scores = [1 if b1 == b2 else 0 for b1, b2 in zip(aligned_seq1, aligned_seq2)]\n    \n    # Plot the results\n    plt.figure(figsize=(10, 2))\n    plt.plot(match_scores, color='blue', linestyle='-', linewidth=1)\n    plt.title(\"Sequence Alignment\")\n    plt.xlabel(\"Position\")\n    plt.ylabel(\"Match (1) / Mismatch (0)\")\n    plt.ylim(-0.1, 1.1)  # Keep the y-axis tight to match/mismatch values\n    plt.show()\n\n# Accession numbers for EGFP and sfGFP\negfp_accession = \"U55762\"\nsfgfp_accession = \"EU304438\"\n\n# Download sequences\negfp_seq = download_sequence(egfp_accession)\nsfgfp_seq = download_sequence(sfgfp_accession)\n\n# Compare sequences\nalignment = compare_sequences(egfp_seq, sfgfp_seq)\n\n# Plot the alignment graphically\nplot_alignment(alignment)\n\n\n\n5.3.6 Multiple sequence alignment objects\n\nhttp://biopython.org/DIST/docs/tutorial/Tutorial.html#htoc70\n\nBio.AlignIO.read() returns a single MultipleSeqAlignment object\n\nBio.AlignIO.parse() returns MultipleSeqAlignment objects\n\n\nfrom Bio import Entrez\nEntrez.email = \"kimhaseong@gmail.com\"\n\n# Searching NCBI Nucleotide database\nhandle = Entrez.esearch(db=\"nucleotide\", term=\"esterase[All Fields] AND \\\"Escherichia coli\\\"[Primary Organism] AND (\\\"580\\\"[SLEN] : \\\"600\\\"[SLEN])\", idtype=\"acc\")\nrecord = Entrez.read(handle)\nprint(record[\"IdList\"])\n\n['NZ_CANUGE010000080.1', 'NZ_CANUHA010000080.1', 'NZ_CANUFW010000086.1', 'NZ_JAINSC010000053.1', 'NZ_JAUOTQ010000344.1', 'NZ_RQOE01000407.1', 'RQOE01000407.1', 'QFSK01000273.1', 'PTNY01001592.1', 'NSEG01000063.1', 'KP965724.1', 'AZLZ01000386.1']\n\n\n\nfrom Bio import SeqIO\n\nmyidx = [0, 4, 5]\nentids = [record[\"IdList\"][x] for x in myidx]\nentids\n\nstream = Entrez.efetch(db=\"nucleotide\", rettype=\"fasta\", retmode=\"text\", id=entids)\n\n#write to an output file\nwith open(\"data/esterase.fasta\", \"w\") as output:\n    output.write(stream.read())\n\nstream.close()\n\n\nAlignment tools\nClustalw를 이용한 서열 정렬 (cactus family Opuntia(선인장))\n\n\n!sudo apt-get update\n!sudo apt-get install -y clustalw\n\n\n!clustalw -help\n\n\n\n\n CLUSTAL 2.1 Multiple Sequence Alignments\n\n\n                DATA (sequences)\n\n-INFILE=file.ext                             :input sequences.\n-PROFILE1=file.ext  and  -PROFILE2=file.ext  :profiles (old alignment).\n\n\n                VERBS (do things)\n\n-OPTIONS            :list the command line parameters\n-HELP  or -CHECK    :outline the command line params.\n-FULLHELP           :output full help content.\n-ALIGN              :do full multiple alignment.\n-TREE               :calculate NJ tree.\n-PIM                :output percent identity matrix (while calculating the tree)\n-BOOTSTRAP(=n)      :bootstrap a NJ tree (n= number of bootstraps; def. = 1000).\n-CONVERT            :output the input sequences in a different file format.\n\n\n                PARAMETERS (set things)\n\n***General settings:****\n-INTERACTIVE :read command line, then enter normal interactive menus\n-QUICKTREE   :use FAST algorithm for the alignment guide tree\n-TYPE=       :PROTEIN or DNA sequences\n-NEGATIVE    :protein alignment with negative values in matrix\n-OUTFILE=    :sequence alignment file name\n-OUTPUT=     :CLUSTAL(default), GCG, GDE, PHYLIP, PIR, NEXUS and FASTA\n-OUTORDER=   :INPUT or ALIGNED\n-CASE        :LOWER or UPPER (for GDE output only)\n-SEQNOS=     :OFF or ON (for Clustal output only)\n-SEQNO_RANGE=:OFF or ON (NEW: for all output formats)\n-RANGE=m,n   :sequence range to write starting m to m+n\n-MAXSEQLEN=n :maximum allowed input sequence length\n-QUIET       :Reduce console output to minimum\n-STATS=      :Log some alignents statistics to file\n\n***Fast Pairwise Alignments:***\n-KTUPLE=n    :word size\n-TOPDIAGS=n  :number of best diags.\n-WINDOW=n    :window around best diags.\n-PAIRGAP=n   :gap penalty\n-SCORE       :PERCENT or ABSOLUTE\n\n\n***Slow Pairwise Alignments:***\n-PWMATRIX=    :Protein weight matrix=BLOSUM, PAM, GONNET, ID or filename\n-PWDNAMATRIX= :DNA weight matrix=IUB, CLUSTALW or filename\n-PWGAPOPEN=f  :gap opening penalty        \n-PWGAPEXT=f   :gap opening penalty\n\n\n***Multiple Alignments:***\n-NEWTREE=      :file for new guide tree\n-USETREE=      :file for old guide tree\n-MATRIX=       :Protein weight matrix=BLOSUM, PAM, GONNET, ID or filename\n-DNAMATRIX=    :DNA weight matrix=IUB, CLUSTALW or filename\n-GAPOPEN=f     :gap opening penalty        \n-GAPEXT=f      :gap extension penalty\n-ENDGAPS       :no end gap separation pen. \n-GAPDIST=n     :gap separation pen. range\n-NOPGAP        :residue-specific gaps off  \n-NOHGAP        :hydrophilic gaps off\n-HGAPRESIDUES= :list hydrophilic res.    \n-MAXDIV=n      :% ident. for delay\n-TYPE=         :PROTEIN or DNA\n-TRANSWEIGHT=f :transitions weighting\n-ITERATION=    :NONE or TREE or ALIGNMENT\n-NUMITER=n     :maximum number of iterations to perform\n-NOWEIGHTS     :disable sequence weighting\n\n\n***Profile Alignments:***\n-PROFILE      :Merge two alignments by profile alignment\n-NEWTREE1=    :file for new guide tree for profile1\n-NEWTREE2=    :file for new guide tree for profile2\n-USETREE1=    :file for old guide tree for profile1\n-USETREE2=    :file for old guide tree for profile2\n\n\n***Sequence to Profile Alignments:***\n-SEQUENCES   :Sequentially add profile2 sequences to profile1 alignment\n-NEWTREE=    :file for new guide tree\n-USETREE=    :file for old guide tree\n\n\n***Structure Alignments:***\n-NOSECSTR1     :do not use secondary structure-gap penalty mask for profile 1 \n-NOSECSTR2     :do not use secondary structure-gap penalty mask for profile 2\n-SECSTROUT=STRUCTURE or MASK or BOTH or NONE   :output in alignment file\n-HELIXGAP=n    :gap penalty for helix core residues \n-STRANDGAP=n   :gap penalty for strand core residues\n-LOOPGAP=n     :gap penalty for loop regions\n-TERMINALGAP=n :gap penalty for structure termini\n-HELIXENDIN=n  :number of residues inside helix to be treated as terminal\n-HELIXENDOUT=n :number of residues outside helix to be treated as terminal\n-STRANDENDIN=n :number of residues inside strand to be treated as terminal\n-STRANDENDOUT=n:number of residues outside strand to be treated as terminal \n\n\n***Trees:***\n-OUTPUTTREE=nj OR phylip OR dist OR nexus\n-SEED=n        :seed number for bootstraps.\n-KIMURA        :use Kimura's correction.   \n-TOSSGAPS      :ignore positions with gaps.\n-BOOTLABELS=node OR branch :position of bootstrap values in tree display\n-CLUSTERING=   :NJ or UPGMA\n\n\n\n!cat data/esterase.fasta\n\n&gt;NZ_CANUGE010000080.1 Escherichia coli strain ROAR-416 / O18:H7 / fimH15 / 95 (ST Warwick) isolate Faeces NODE_80_length_585_cov_58.954717, whole genome shotgun sequence\nGGCGGCCTCTATACCCGCTTATGGCATGACAGCGTCAGCAGTACTGCGCTCCATCGCCAGCACAACATGA\nAGGAGGAAACCCCGGGATAGTTACTGGACACGTAATGTATTAAAAACACAGTCAGAAGCGGCGGTACCGT\nGAATAGCCGCTTTAATTATTTATACTGACATCCTTAATTTTTAAAGAGTATGAATGCTGAACATGCAACA\nACATCCCTCTGCTATCGCCAGCCTGCGCAACCAACTGGCAGCGGGCCACATTGCTAACCTTACTGACTTC\nTGGCGCGAAGCTGAGTCGCTGAATGTTCCTCTTGTGACGCCAGTCGAAGGAGCGGAAGATGAGCGAGAAG\nTGACCTTTCTGTGGCGCGCCCGACATCCTCTGCAGGGCGTTTATCTGCGTCTGAACCGGGTGACGGATAA\nAGAGCACGTAGAAAAAGGAATGATGAGCGCCCTTCCCGAAACGGATATCTGGACACTGACACTGCGTTTA\nCCCGCAAGTTACTGCGGCTCCTATTCGCTGCTGGAAATCCCCCCCGGCACTACGGCTGAGACGATTGCAC\nTGTCCGGAGGCCGTTTTGCCACCCT\n\n&gt;NZ_JAUOTQ010000344.1 Escherichia coli strain SCL2922 NODE_344_length_581_cov_2.811508, whole genome shotgun sequence\nGAATGGAACGGCGGCTTCCACACCGGACAACTGCTTACCTCCATGCGCATTATCGCCGGGAAATCTCGCC\nAGGTTCGGCTCTATATTCCGGACGTTGATATTTCTCAGCCCCTCGGGCTGGTCGTGCTGCCCGATGGTGA\nAACCTGGTTTGATCACCTTGGCGTATGCGCGGCAATTGACGCCGCCATAAATAACAGGCGCATCGTGCCC\nGTGGCTGTACTGGGCATTGACAACATTAATGAACATGAACGCACTGAGATACTCGGCGGGCGCAGCAAGC\nTGATAAAGGATATCGCAGGACATCTGCTGCCGATGATCCGCGCTGAACAACCGCAGCGTCAGTGGGCAGA\nCCGTTCGCGCACAGTGCTGGCCGGGCAGAGCCTCGGCGGGATCAGTGCACTAATGGGGGCTCGTTACGCA\nCCGGAAACGTTCGGTCTGGTGCTCAGCCACTCTCCTTCAATGTGGTGGACGCCAGAAAGAACCAGTCTAC\nCAGGCTTGTTCAGCGAAACCGATACCTCATGGGTGAGTGAGCATCTGCTTTCTGCCCCACCGCAGGGCGT\nGCGTATCAGCCTGTGCGTGGG\n\n&gt;NZ_RQOE01000407.1 Escherichia coli strain S366 NODE_408_length_581_cov_1.19604, whole genome shotgun sequence\nACTGAATATCGCTCGGCAACGCCGCGGCTTATGGGGGCACTCCTACGGCGGCCTCTTCGTGCTGGATTCC\nTGGCTGTCCTCCTCTTACTTCCGGTCGTACTACAGCGCCAGCCCGTCGTTGGGCAGAGGTTATGATGCTT\nTGCTAAGCCGCGTTACGGCGGTTGAGCCTCTGCAATTCTGCGCCAAACACCTGGCGATAATGGAAGGCTC\nGGCGACACAGGGTGATAACCGGGAAACGCATGCTGTCGGGGTGCTGTCGAAAATTCATACCACCCTCACT\nATACTGAAAGATAAAGGCGTCAATGCCGTATTTTGGGATTTCCCCAACCTAGGACACGGGCCGATGTTCA\nATGCCTCCTTTCGCCAGGCACTGTTAGATATCAGTGGTGAAAACGCAAATTACACAGCAGGTTGTCATGA\nGTTAAGCCACTAAACACTGCCCGCTTTTACGCGGGCAGTACGCCTGAAACACTACGATCAGAATGATGCG\nGTAACTCCGGCATAGTAAGCCCGGCCTGGCTCGTTATAGGTATTCGCCCCTTCAGAAGATCGGAAGATCT\nGTTTATTGAGGATATTACTGA\n\n\n\n\n!clustalw -infile=data/esterase.fasta -outfile=data/esterase.aln\n\n\n\n\n CLUSTAL 2.1 Multiple Sequence Alignments\n\n\nSequence format is Pearson\nSequence 1: NZ_CANUGE010000080.1   585 bp\nSequence 2: NZ_JAUOTQ010000344.1   581 bp\nSequence 3: NZ_RQOE01000407.1      581 bp\nStart of Pairwise alignments\nAligning...\n\nSequences (1:2) Aligned. Score:  4\nSequences (1:3) Aligned. Score:  4\nSequences (2:3) Aligned. Score:  3\nGuide tree file created:   [data/esterase.dnd]\n\nThere are 2 groups\nStart of Multiple Alignment\n\nAligning...\nGroup 1:                     Delayed\nGroup 2:                     Delayed\nAlignment Score 3099\n\nCLUSTAL-Alignment file created  [data/esterase.aln]\n\n\n\n\nfrom Bio import AlignIO\nalign = AlignIO.read(\"data/esterase.aln\", \"clustal\")\nprint(align)\n\nAlignment with 3 rows and 641 columns\n--------------GGCGGCCTCTATACCCGCTTATGGCATGAC...CCT NZ_CANUGE010000080.1\n-----GAATGGAACGGCGGCTTCCACACCGGACAACTGCTTACC...--- NZ_JAUOTQ010000344.1\nACTGAATATCGCTCGGCAACG-CCGCGGCTTATGGGGGCACTCC...--- NZ_RQOE01000407.1\n\n\n\nalign.substitutions\n\n\nfrom Bio import Phylo\ntree = Phylo.read(\"data/esterase.dnd\", \"newick\")\nPhylo.draw_ascii(tree)\n\n  ______________________________________________________ NZ_CANUGE010000080.1\n |\n_|_______________________________________________________ NZ_JAUOTQ010000344.1\n |\n |________________________________________________________ NZ_RQOE01000407.1\n\n\n\n\n5.3.6.1 Position specific scoring matrix (PSSM) matrix\n\nfrom Bio import AlignIO\nfrom Bio.Align import AlignInfo\n\n\nPfam은 단백질 페밀리 database, 각 서열 그룹을 align 한 파일이 제공됨\nFamily: Sigma54_activ_2 (PF14532) https://www.ebi.ac.uk/interpro/entry/pfam/PF14532/\n\n\nalign = AlignIO.read(\"/content/drive/MyDrive/2024-cnu-lecture/data/PF14532_full.txt\", \"stockholm\")\nprint(align)\nprint(len(align))\n\nAlignment with 1240 rows and 765 columns\n--------------------------------------------...--- A0A1F7TK17_9BACT/134-279\n--------------------------------------------...--- A0A1H8MG25_9RHOB/297-456\n--------------------------------------------...--- A0A0P1IVG1_9RHOB/141-283\n--------------------------------------------...--- V7EPJ0_9RHOB/141-283\n--------------------------------------------...--- B1ZTM1_OPITP/145-296\n--------------------------------------------...--- W3ANH6_9FIRM/219-355\n--------------------------------------------...--- Q6LNI3_PHOPR/144-289\n--------------------------------------------...--- A0A1G8U4Y5_9RHOB/145-284\n--------------------------------------------...--- W1HPT9_KLEPN/143-283\n--------------------------------------------...--- A0A0D6TAT1_9RHOB/146-292\n--------------------------------------------...--- A0A1M4UVA9_9CLOT/309-454\n--------------------------------------------...--- A0A252BQ85_9PROT/8-137\n--------------------------------------------...--- A0A0P1G5P1_9RHOB/143-285\n--------------------------------------------...--- A0A1M6RYM0_PSETH/338-463\n--------------------------------------------...--- A0A0P7EE64_9GAMM/139-307\n--------------------------------------------...--- A0A1I6GDU4_9RHOB/146-282\n--------------------------------------------...--- A0A1G0NB92_9PROT/137-274\n--------------------------------------------...--- A0A0C5W5C5_9GAMM/145-290\n...\n--------------------------------------------...--- A0A2A4B1F3_9SPHN/145-294\n1240\n\n\n\nslicing alignment\n\n\nprint(align[3:8,100:200])\n\nAlignment with 5 rows and 100 columns\n--V---A---R---V---M--------N---T----D-------...K-- V7EPJ0_9RHOB/141-283\n--V---K---K---L---A--------A---V----R-------...E-- B1ZTM1_OPITP/145-296\n--A---E---K---L---S--------R---T----D-------...N-- W3ANH6_9FIRM/219-355\n--I---A---N---I---A--------L---T----N-------...K-- Q6LNI3_PHOPR/144-289\n--V---R---L---V---A--------R---A----G-------...E-- A0A1G8U4Y5_9RHOB/145-284\n\n\n\nTurn the alignment object into an array of letters\n\n\nimport numpy as np\nfrom Bio import AlignIO\nalign = AlignIO.read(\"/content/drive/MyDrive/2024-cnu-lecture/data/esterase.aln\", \"clustal\")\n\n# convert to character array\nalign_array = np.array([list(rec) for rec in align], 'U')\nprint(align_array)\n\n[['-' '-' '-' ... 'C' 'C' 'T']\n ['-' '-' '-' ... '-' '-' '-']\n ['A' 'C' 'T' ... '-' '-' '-']]\n(3, 641)\n\n\nNote that this leaves the original Biopython alignment object and the NumPy array in memory as separate objects - editing one will not update the other!\n\nalign_array.shape\n\n(3, 641)\n\n\n\nSummaryInfo 클래스\nconsensus sequence, position specific score matrix 계산\ninformation content와 substitution 정보 계산 가능\n\n\nsummary_align = AlignInfo.SummaryInfo(align)\nconsensus = summary_align.dumb_consensus()\nprint(consensus)\nmy_pssm = summary_align.pos_specific_score_matrix(consensus, chars_to_ignore = ['N', '-'])\n#print(my_pssm)\n# your_pssm[sequence_number][residue_count_name]\n[s for s in my_pssm][:20]\n\nACTGAXXATXGXXCGGCXXCXTCXXXXXCXXXXXXXXGCXXXXCXXCXXXXGCXXXXXXGXXXXGXXXXXXXGXCXGXXXXXXXXXXAXXXXXXGXXXXXXXAXAXXXCXXXXXXXXXCGGXXTGGXCXXXXXXXXXXXTXXTGXXXCXXXGXXXXXXXACXXXGGXXXXXXCXXXGCAATXGXXGCXXXXAXXAXXXXXXXXXXXATXGXXXXXXXXXXXXXAXXGXGXXXXXAXCGGXXXXXXXATGCXXXXXXXXXXCXXTCGXXXATXXXXXXCXXXCXCAXXXXXCTGXXAXXXXXXXXCXXXXXXXXXXTXXTXXXGGAXXXXXXXXXCXXAXXAXXCGXXXXGXXXXTXXXXXXCXCCXXTXGXXXXXXXXXXXXXXXXXXCXGXXXXXXXXXXXXAAXTXAXXXXXCXXXXXXXXXXXXGXXAXXCXXXXXAXXXXXXXXXXCTXXXXCXXXXXCXXGTXXCXXXXXAXXXXCACGXXXXXXXXXGAAXGATGAXCXXXXXXXCXGXXXXXXXXAXCXXGXXACXGXXXCXXXXTXXXXXXGXXXGXXXXXGCXXXTXXXXXXXXXXGXXXGXXXTXXXXXXXXXXXXTXXXXXTGXGACGATTGCACTGTCCGGAGGCCGTTTTGCCACCCT\n\n\n[{'A': 1.0, 'C': 0, 'G': 0, 'T': 0},\n {'A': 0, 'C': 1.0, 'G': 0, 'T': 0},\n {'A': 0, 'C': 0, 'G': 0, 'T': 1.0},\n {'A': 0, 'C': 0, 'G': 1.0, 'T': 0},\n {'A': 1.0, 'C': 0, 'G': 0, 'T': 0},\n {'A': 1.0, 'C': 0, 'G': 1.0, 'T': 0},\n {'A': 1.0, 'C': 0, 'G': 0, 'T': 1.0},\n {'A': 2.0, 'C': 0, 'G': 0, 'T': 0},\n {'A': 0, 'C': 0, 'G': 0, 'T': 2.0},\n {'A': 0, 'C': 1.0, 'G': 1.0, 'T': 0},\n {'A': 0, 'C': 0, 'G': 2.0, 'T': 0},\n {'A': 1.0, 'C': 1.0, 'G': 0, 'T': 0},\n {'A': 1.0, 'C': 0, 'G': 0, 'T': 1.0},\n {'A': 0, 'C': 2.0, 'G': 0, 'T': 0},\n {'A': 0, 'C': 0, 'G': 3.0, 'T': 0},\n {'A': 0, 'C': 0, 'G': 3.0, 'T': 0},\n {'A': 0, 'C': 3.0, 'G': 0, 'T': 0},\n {'A': 1.0, 'C': 0, 'G': 2.0, 'T': 0},\n {'A': 1.0, 'C': 0, 'G': 2.0, 'T': 0},\n {'A': 0, 'C': 3.0, 'G': 0, 'T': 0}]\n\n\n\ninstances = [al.seq for al in align[:10]]\nprint(instances)\n\n[Seq('--------------GGCGGCCTCTATACCCGCTTATGGCATGACAGCGTCAGCA...CCT'), Seq('-----GAATGGAACGGCGGCTTCCACACCGGACAACTGCTTACCTCCATGCGCA...---'), Seq('ACTGAATATCGCTCGGCAACG-CCGCGGCTTATGGGGGCACTCCTACGGCGGCC...---')]\n\n\n\n\n5.3.6.2 Motif\n\nBio.motifs package included in Biopython 1.61\n\n\nfrom Bio import motifs\nfrom Bio.Seq import Seq\n\n\ninstances = [Seq(\"TACAA\"),\n            Seq(\"TACGA\"),\n            Seq(\"TACAA\"),\n            Seq(\"TAGAA\"),\n            Seq(\"TACAA\"),\n            Seq(\"AACGA\"),\n           ]\n\n\nm = motifs.create(instances)\nprint(m)\n\nTACAA\nTACGA\nTACAA\nTAGAA\nTACAA\nAACGA\n\n\n\nm.counts\n\n{'A': [1.0, 6.0, 0.0, 4.0, 6.0],\n 'C': [0.0, 0.0, 5.0, 0.0, 0.0],\n 'G': [0.0, 0.0, 1.0, 2.0, 0.0],\n 'T': [5.0, 0.0, 0.0, 0.0, 0.0]}\n\n\n\nm.counts[\"A\", 1]\nr = m.reverse_complement()\nprint(r.consensus)\nr.weblogo(\"data/mymotif.png\")\n\nTTGTA\n\n\n\nfrom IPython.display import Image, display\ndisplay(Image(filename=\"/content/drive/MyDrive/2024-cnu-lecture/data/mymotif.png\"))\n\n\n\n\n\n\n\n\n\nPosition-weight matrices 계산\n.counts 특성 사용\n\n\npwm = m.counts.normalize(pseudocounts=0.5)\nprint(pwm)\n\n        0      1      2      3      4\nA:   0.19   0.81   0.06   0.56   0.81\nC:   0.06   0.06   0.69   0.06   0.06\nG:   0.06   0.06   0.19   0.31   0.06\nT:   0.69   0.06   0.06   0.06   0.06\n\n\n\n\npssm = pwm.log_odds()\nprint(pssm)\n\n        0      1      2      3      4\nA:  -0.42   1.70  -2.00   1.17   1.70\nC:  -2.00  -2.00   1.46  -2.00  -2.00\nG:  -2.00  -2.00  -0.42   0.32  -2.00\nT:   1.46  -2.00  -2.00  -2.00  -2.00\n\n\n\n\nbackground = {\"A\":0.3,\"C\":0.2,\"G\":0.2,\"T\":0.3}\npssm = pwm.log_odds(background)\nprint(pssm)\n\n        0      1      2      3      4\nA:  -0.68   1.44  -2.26   0.91   1.44\nC:  -1.68  -1.68   1.78  -1.68  -1.68\nG:  -1.68  -1.68  -0.09   0.64  -1.68\nT:   1.20  -2.26  -2.26  -2.26  -2.26\n\n\n\n\nUsing the PSSM score\n\n\nfor pos, score in pssm.search(test_seq, threshold=3.0):\n    print(\"%d, %f \" % (pos, score))\nprint(pssm.calculate(test_seq))\n\n0, 3.643981 \n10, 6.759458 \n[  3.643981    -8.560285    -2.4004133   -5.6533937   -4.2748823\n  -0.05645879 -10.145247    -3.3293302   -5.9753222   -3.5703382\n   6.759458    -5.3903594   -5.8598447   -0.81545067  -0.81545067\n   0.7695118   -6.3903594   -3.5379167    0.4255574   -1.9309279\n -10.145247    -3.3293302 ]\n\n\n\n\nm.pseudocounts = 0.1\nprint(m.counts)\nprint(m.pwm)\nprint(m.pssm)\n\n        0      1      2      3      4\nA:   1.00   6.00   0.00   4.00   6.00\nC:   0.00   0.00   5.00   0.00   0.00\nG:   0.00   0.00   1.00   2.00   0.00\nT:   5.00   0.00   0.00   0.00   0.00\n\n        0      1      2      3      4\nA:   0.17   0.95   0.02   0.64   0.95\nC:   0.02   0.02   0.80   0.02   0.02\nG:   0.02   0.02   0.17   0.33   0.02\nT:   0.80   0.02   0.02   0.02   0.02\n\n        0      1      2      3      4\nA:  -0.54   1.93  -4.00   1.36   1.93\nC:  -4.00  -4.00   1.67  -4.00  -4.00\nG:  -4.00  -4.00  -0.54   0.39  -4.00\nT:   1.67  -4.00  -4.00  -4.00  -4.00\n\n\n\n\n\n\n5.3.7 scikit-bio\nscikit-bio is another library focused on bioinformatics, providing functionalities for sequence analysis, alignment, phylogenetics, and more.\n\n5.3.7.1 Installation\npip install scikit-bio\n\n\n5.3.7.2 Basic Usage Examples\nReading and Writing Sequence Files:\nimport skbio\n\n# Reading a FASTA file\nfor seq in skbio.io.read(\"example.fasta\", format=\"fasta\"):\n    print(seq.metadata['id'])\n    print(seq)\n\n# Writing to a FASTA file\nsequences = [seq for seq in skbio.io.read(\"example.fasta\", format=\"fasta\")]\nskbio.io.write(sequences, \"output.fasta\", format=\"fasta\")\nSequence Analysis:\nfrom skbio import DNA, RNA, Protein\n\n# Creating sequences\ndna_seq = DNA(\"AGTACACTGGT\")\nrna_seq = dna_seq.transcribe()\nprotein_seq = dna_seq.translate()\n\nprint(\"DNA: \", dna_seq)\nprint(\"RNA: \", rna_seq)\nprint(\"Protein: \", protein_seq)\nAlignment:\nfrom skbio import DNA\nfrom skbio.alignment import local_pairwise_align_ssw\n\nseq1 = DNA(\"ACTGCTAGCTAG\")\nseq2 = DNA(\"GCTAGCTAGGTA\")\n\nalignment, score, start_end_positions = local_pairwise_align_ssw(seq1, seq2)\n\nprint(\"Alignment:\\n\", alignment)\nprint(\"Score:\", score)\nprint(\"Start-End Positions:\", start_end_positions)\nPhylogenetic Tree Construction:\nfrom skbio import DistanceMatrix\nfrom skbio.tree import nj\n\n# Example distance matrix\ndm = DistanceMatrix([[0.0, 0.2, 0.4],\n                     [0.2, 0.0, 0.6],\n                     [0.4, 0.6, 0.0]],\n                    ['A', 'B', 'C'])\n\n# Constructing a phylogenetic tree using Neighbor-Joining\ntree = nj(dm)\nprint(tree.ascii_art())\n\n\n\n5.3.8 Example: Analyzing a Protein Sequence\nLet’s put together an example that reads a protein sequence from a GenBank file, performs some basic analysis using Biopython, and aligns it using scikit-bio.\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom skbio import Protein\nfrom skbio.alignment import global_pairwise_align_protein\n\n# Read a protein sequence from a GenBank file\nrecord = SeqIO.read(\"example.gb\", \"genbank\")\nprotein_seq = record.seq.translate()\n\nprint(\"Protein Sequence:\", protein_seq)\n\n# Perform reverse translation to get the DNA sequence\ndna_seq = protein_seq.reverse_translate(table=11)\nprint(\"Reverse Translated DNA Sequence:\", dna_seq)\n\n# Perform alignment with another protein sequence\nseq1 = Protein(str(protein_seq))\nseq2 = Protein(\"MKVLYNLKDG\")\n\nalignment, score, start_end_positions = global_pairwise_align_protein(seq1, seq2)\n\nprint(\"Alignment:\\n\", alignment)\nprint(\"Score:\", score)\nprint(\"Start-End Positions:\", start_end_positions)\n\n\n5.3.9 Explanation:\n\nReading Sequence: We use Biopython to read a protein sequence from a GenBank file.\nSequence Analysis: We perform basic sequence operations like translation and reverse translation.\nAlignment: We use scikit-bio to perform a global pairwise alignment between two protein sequences.\n\nBy mastering these bioinformatics libraries, you will be well-equipped to handle a wide range of bioinformatics tasks and analyses in your research. If you have specific questions or need further examples, feel free to ask!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Bioinformatics</span>"
    ]
  },
  {
    "objectID": "cnu_machine_learning.html",
    "href": "cnu_machine_learning.html",
    "title": "6  Machine Learning",
    "section": "",
    "text": "6.1 K-Nearest Neighbor (KNN)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nbreast_cancer = load_breast_cancer()\ndir(breast_cancer)\nprint(breast_cancer.DESCR)\nprint(breast_cancer.data.shape)\nX = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\nX.head()\n\n(569, 30)\n\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\ny = pd.Categorical.from_codes(breast_cancer.target, breast_cancer.target_names)\ny = pd.get_dummies(y, drop_first=True)\ny\n\n\n\n\n\n\n\n\n\nbenign\n\n\n\n\n0\nFalse\n\n\n1\nFalse\n\n\n2\nFalse\n\n\n3\nFalse\n\n\n4\nFalse\n\n\n...\n...\n\n\n564\nFalse\n\n\n565\nFalse\n\n\n566\nFalse\n\n\n567\nFalse\n\n\n568\nTrue\n\n\n\n\n569 rows × 1 columns\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nsns.scatterplot(\n    x='mean area',\n    y='mean compactness',\n    hue='benign',\n    data=X_test.join(y_test, how='outer')\n)\n\n/home/haseong/anaconda3/envs/biopy/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  return self._fit(X, y)\nresults = pd.DataFrame([y_test[\"benign\"].values, y_pred], index=(\"test\", \"predict\")).T\nprint(results)\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\nAccuracy: 0.9370629370629371\nfrom sklearn.inspection import permutation_importance\n\n# Compute permutation feature importance\nresults = permutation_importance(knn, X_test, y_test, n_repeats=5, random_state=40)\n\n# Get importance scores\nfeature_importance = results.importances_mean\n\n# Plot feature importance\nplt.bar(range(X.shape[1]), feature_importance, tick_label=breast_cancer.feature_names)\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.title('Feature Importance Using Permutation on Model')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "cnu_machine_learning.html#k-nearest-neighbor-knn",
    "href": "cnu_machine_learning.html#k-nearest-neighbor-knn",
    "title": "6  Machine Learning",
    "section": "",
    "text": "KNN은 지도학습의 한 종류로 거리 기반의 분류모델임\n비지도학습의 클러스터링과 유사하나 Y값 (class, 반응값)이 있다는 점에서 클러스터링과 차이\n설명변수 X 값들을 이용해 반응변수 Y (분류)값을 설명하는 모형으로\n새로운 데이터가 주어질 경우 K개의 인접한 데이터를 찾아서 이에 속한 클래스로 분류 수행\n\n\n\nhttps://github.com/HongleiXie/MLFromScratch\n각종 ML 모형 (python, tensorflow 프레임워크)\n\n\n\n\n\ndummy y 변수 생성\n\n\n\n데이터셋 준비 (훈련/테스트셋)\n\n\n\n모형 훈련 및 그래프\n\n\n\n예측 성능\n\n\n\n\n변수 중요도 탐색",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "cnu_deeplearning.html",
    "href": "cnu_deeplearning.html",
    "title": "7  Deep Learning",
    "section": "",
    "text": "7.1 Convolutional Neual Network",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  },
  {
    "objectID": "cnu_deeplearning.html#convolutional-neual-network",
    "href": "cnu_deeplearning.html#convolutional-neual-network",
    "title": "7  Deep Learning",
    "section": "",
    "text": "딥러닝 기반 DNA 서열 분석 가이드\nhttp://www.btnews.or.kr/bbs/board.php?bo_table=bt_news&wr_id=342\n\n\n7.1.1 목표\n\n임의의 활성을 갖는 서열을 분류하는 CNN 모형 개발 (시뮬레이션)\n예시) 임의의 전사인자가 결합하는 특정 DNA 모티프 찾는 모형 개발. 즉, 모형 개발 후 임의의 DNA 서열을 모형에 넣었을 때 해당 전사인자가 입력 DNA 서열에 붙으면 1 붙지 않으면 0 이라고 예측하는 모형 개발\n\n\n\n7.1.2 데이터\n딥러닝을 위해서는 라벨링 데이터가 필요함 (최근 self-supervised learning에서는 필수는 아님). 서열분석의 경우에는 DNA 서열과 함께 해당 서열의 표현형이 라벨이 될 수 있음 (Genotype-phenotype 짝 데이터). 예를 들어 특정 전사인자가 결합하는 DNA 서열을 예측하는 딥러닝 모형을 학습하고자 할 경우 전사인자의 서열 데이터와 해당 전사인자가 DNA에 실제로 붙는지를 나타내는 True 또는 False 라벨이 붙은 데이터가 필요함.\n일반적으로 통계적 분석을 위한 데이터는 샘플의 개수와 (행) 변수의 개수로 (열) 구분되어 2차원 배열 형태로 표현. 딥러닝에서도 같은 방식으로 데이터를 표현하며 필요한 샘플의 수는 학습할 모형의 복잡도에 따라서 달라질 수 있지만 최소 수천 개 이상이 필요하며 수 만개 이상의 가능한 많은 데이터를 사용 권장.\n딥러닝을 위해서 수집된 데이터 세트는 모형 학습을 위한 Training 데이터와 Test 데이터로 나누어 사용되며 Training 데이터는 또다시 Training 데이터와 Validation 데이터로 나누어 구분.\n\n\n7.1.3 One-hot encoding\n딥러닝을 위해서 데이터는 숫자(기계가 인식 가능한)로 표현 필요. One-hot encoding은 딥러닝에서 가장 널리 사용되는 방법 중 하나이며 4 종류의 염기를 갖는 DNA의 경우 “A”는 [1,0,0,0], “T”는 [0,0,0,1], “G”는 [0,0,1,0], 그리고 “C”는 [0,1,0,0] 으로 인코딩 할 수 있음\n\nimport numpy as np\n\nmy_string=\"ATACAA\"\nmy_array=np.array(list(my_string))\nprint(my_array)\n\n['A' 'T' 'A' 'C' 'A' 'A']\n\n\n\nlist(my_string)\n\n['A', 'T', 'A', 'C', 'A', 'A']\n\n\n\nNumpy Dimension 관련 내용은 앞서 강의 numpy 부분 참고\n\n\ndisplay(np.zeros(5))\ndisplay(np.zeros((7,5)))\n\narray([0., 0., 0., 0., 0.])\n\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n\nbox = np.zeros((3, 7, 5))\ntype(box)\n\nnumpy.ndarray\n\n\n\nonehot_encode = np.zeros((len(my_array),4), dtype=int)\nbase_dict = {\"A\":0, \"C\":1, \"G\":2, \"T\":3}\nfor i in range(len(my_array)):\n    onehot_encode[i, base_dict[my_array[i]]] = 1\n\nprint(onehot_encode)\nprint(onehot_encode.shape)\n\n[[1 0 0 0]\n [0 0 0 1]\n [1 0 0 0]\n [0 1 0 0]\n [1 0 0 0]\n [1 0 0 0]]\n(6, 4)\n\n\n\none-hot 방식으로 변환한 서열 데이터\n\n\n\n\nalt text\n\n\n\n\n7.1.4 모티프 설정\nPFM (Position Frequency Matrix)와 PWM (Position Weight Matrix) 개념 이해 필요. Alignment가 수행된 몇 개의 서열들을 가정하면 PFM은 이 서열들의 특정 위치에 A, T, G, C 각 염기들의 빈도수를 나타내며 PWM은 각 염기의 비율을 나타냄.\n\nfrom Bio import motifs\nfrom Bio.Seq import Seq\n\ninstances = [Seq(\"TACAA\"), Seq(\"TACGA\"), Seq(\"TACAA\")]\nm = motifs.create(instances)\npfm = m.counts\nprint(pfm)\npwm = m.counts.normalize(pseudocounts=0.5)\nprint (pwm)\n\n        0      1      2      3      4\nA:   0.00   3.00   0.00   2.00   3.00\nC:   0.00   0.00   3.00   0.00   0.00\nG:   0.00   0.00   0.00   1.00   0.00\nT:   3.00   0.00   0.00   0.00   0.00\n\n        0      1      2      3      4\nA:   0.10   0.70   0.10   0.50   0.70\nC:   0.10   0.10   0.70   0.10   0.10\nG:   0.10   0.10   0.10   0.30   0.10\nT:   0.70   0.10   0.10   0.10   0.10\n\n\n\npseudocounts는 계산시 NULL이나 0으로 나누어지는 경우 방지. 특정 서열 모티프의 PWM은 새로운 서열이 One-hot encoding 방식으로 주어져 있을 경우 서열 처음 위치부터 마지막까지 Sliding window 방식으로 해당 모티프가 있는 위치를 탐색할 수 있음. 다음은 위 주어진 pwm모티프를 0, 1로만 가정하여 해당 모티프의 존재 유무를 계산하는 방법을 보여줌.\n\n\n\nalt text\n\n\n위와 같이 길이3 배열에 1이 있을 경우 타깃 서열에 모티프와 같은 서열이 있음을 알 수 있음\n앞서 “ATACAA” 서열에서 위 PWM 모티프가 존재하는지 탐색. “ATACAA” 는 길이 5인 슬라이딩 윈도우를 사용하면 “ATACA”와 “TACAA” 두 개의 서열로 나눌 수 있음. 이 두 서열을 One-hot encoding으로 전환 후 위 PWM과 원소들끼리 곱하면 One-hot encoding에서 0이 아닌 위치와 동일 위치의 PWM 값들만 남게 되므로 0이 아닌 값들을 모두 곱한 후 log를 취해 주면 해당 서열이 모티프와 얼마나 비슷한지를 나타내는 스칼라 값이 구해짐. 이론적으로 이 값이 0이면 동일한 서열임.\n\npwm_arr = np.array(list(pwm.values())).transpose()\nprint(pwm_arr.shape)\n\nprint(onehot_encode.shape)\nprint(onehot_encode[0:5,].shape)\nprint(onehot_encode[1:6,].shape)\n\ns1 = np.multiply(onehot_encode[0:5,], pwm_arr)\ns2 = np.multiply(onehot_encode[1:6,], pwm_arr)\nprint(s1)\nprint(s2)\n\nprint(np.sum(s1, axis=1))\nprint(np.prod(np.sum(s1, axis=1)))\n\nprint(np.log(np.prod(np.sum(s1, axis=1)))) #s1 score\nprint(np.log(np.prod(np.sum(s2, axis=1)))) #s2 score\n\n(5, 4)\n(6, 4)\n(5, 4)\n(5, 4)\n[[0.1 0.  0.  0. ]\n [0.  0.  0.  0.1]\n [0.1 0.  0.  0. ]\n [0.  0.1 0.  0. ]\n [0.7 0.  0.  0. ]]\n[[0.  0.  0.  0.7]\n [0.7 0.  0.  0. ]\n [0.  0.7 0.  0. ]\n [0.5 0.  0.  0. ]\n [0.7 0.  0.  0. ]]\n[0.1 0.1 0.1 0.1 0.7]\n7.000000000000002e-05\n-9.567015315914915\n-2.119846956314875\n\n\n\n딥러닝 스타일로 배열을 가시화 할 경우 다음과 같이 표현 가능\n\n\n\n\nalt text\n\n\n\n\n7.1.5 모의 서열 데이터 생성\n서열 중간 motif를 넣어서 임의의 시뮬레이션 positive 데이터를 1000개 생성하고 랜덤한 서열을 넣어 negative 데이터를 1000개 생성함\n\nimport numpy as np\nseq_length = 20\nnum_sample = 1000\n#motif CCGGAA\nmotif_pwm = np.array([[10.41, 22.86, 1.92, 1.55, 98.60, 86.66],\n            [68.20, 65.25, 0.50, 0.35, 0.25, 2.57],\n            [17.27, 8.30, 94.77, 97.32, 0.87, 0.00],\n            [4.13, 3.59, 2.81, 0.78, 0.28, 10.77]])\npwm = np.hstack([np.ones((4, 7)), motif_pwm, np.ones((4, 7))])\npos = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=pwm[:,i]/sum(pwm[:,i])) for i in range(seq_length)]).transpose()\nneg = np.array([np.random.choice( ['A', 'C', 'G', 'T'], num_sample,\n                                  p=np.array([1,1,1,1])/4) for i in range(seq_length)]).transpose()\n\nprint(pos.shape)\ndisplay([''.join(x) for x in pos[1:5,]])\nprint()\ndisplay([''.join(x) for x in neg[1:5,]])\n\n(1000, 20)\n\n\n\n['AGCGTAGGCGGAACATAATA',\n 'GGATCGTCAGGATCACCGCC',\n 'CCGAGTCACGGAATTAACTG',\n 'AACATCAGCGGAAGCTTTGT']\n\n\n['TTCCAATTACCGACCTGGAT',\n 'ATTGATTTCCTGCCAAGATC',\n 'AGTGAGCTGCTTTAGGTCCC',\n 'TGTGAGGGCTTAGATGAATG']\n\n\n\n\n7.1.6 DNA 서열 데이터 전처리\n\nbase_dict = {'A':0, 'C':1, 'G':2, 'T':3}\n\n# response variable for pos\nonehot_encode_pos = np.zeros((num_sample, seq_length, 4))\nonehot_encode_pos_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_pos_label[:,0] = 1\n# print(onehot_encode_pos_label)\n\n# response variable for pos\nonehot_encode_neg = np.zeros((num_sample, seq_length, 4))\nonehot_encode_neg_label = np.zeros((num_sample, 2), dtype=int)\nonehot_encode_neg_label[:,1] = 1\n# print(onehot_encode_neg_label)\n\n# convert sequence to onehot\nfor i in range(num_sample):\n    for j in range(seq_length):\n        onehot_encode_pos[i,j,base_dict[pos[i,j]]] = 1\n        onehot_encode_neg[i,j,base_dict[neg[i,j]]] = 1\n\n# concatenation\nX = np.vstack((onehot_encode_pos, onehot_encode_neg))\ny = np.vstack((onehot_encode_pos_label, onehot_encode_neg_label))\n\nprint(X.shape, y.shape)\n# (2000, 20, 4) (2000, 2)\n\n(2000, 20, 4) (2000, 2)\n\n\n\nPyTorch Conv1d는 입력 데이터가 [batch_size, channels, length]의 형식이므로 transpose(1,2) 적용\n\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# 데이터를 훈련 세트와 테스트 세트로 나눔\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=125)\nprint(X_train.shape, y_train.shape)\n\n# NumPy 배열을 PyTorch 텐서로 변환\nX_train = torch.tensor(X_train, dtype=torch.float32).transpose(1,2)\nX_test = torch.tensor(X_test, dtype=torch.float32).transpose(1,2)\ny_train = torch.tensor(y_train, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\nprint(y_test.dtype)\n\n# DataLoader 설정\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nprint(train_loader.dataset.tensors[0].shape)\nprint(train_loader.dataset.tensors[1].shape)\ntest_dataset = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n(1600, 20, 4) (1600, 2)\ntorch.float32\ntorch.Size([1600, 4, 20])\ntorch.Size([1600, 2])\n\n\n\nimport torch\n\nX_torch = torch.tensor(X_train, dtype=torch.float32)\nprint(X_torch.shape)\n\ntorch.Size([1600, 4, 20])\n\n\n/tmp/ipykernel_341094/3124571761.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  X_torch = torch.tensor(X_train, dtype=torch.float32)\n\n\n\n\n7.1.7 모델 정의\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DNA_CNN(nn.Module):\n    def __init__(self):\n        super(DNA_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool1d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(160, 64)  # Adjust the input features according to your pooling and conv1d output\n        self.fc2 = nn.Linear(64, 2)  # Adjust according to your problem's needs (e.g., number of classes)\n        #self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        #x = self.softmax(x)\n        return x\n\nmodel = DNA_CNN()\nif torch.cuda.is_available():\n    model.cuda()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfrom torchsummary import summary\nsummary(model, input_size=(4, 20))  # (Channels, Length)\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv1d-1               [-1, 16, 20]             208\n              ReLU-2               [-1, 16, 20]               0\n         MaxPool1d-3               [-1, 16, 10]               0\n           Flatten-4                  [-1, 160]               0\n            Linear-5                   [-1, 64]          10,304\n            Linear-6                    [-1, 2]             130\n================================================================\nTotal params: 10,642\nTrainable params: 10,642\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.05\n----------------------------------------------------------------\n\n\n\n\n7.1.8 훈련\n\n# 훈련 루프\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n\nEpoch [1/20], Loss: 0.4758\nEpoch [2/20], Loss: 0.1928\nEpoch [3/20], Loss: 0.0787\nEpoch [4/20], Loss: 0.0622\nEpoch [5/20], Loss: 0.0541\nEpoch [6/20], Loss: 0.0364\nEpoch [7/20], Loss: 0.0975\nEpoch [8/20], Loss: 0.0732\nEpoch [9/20], Loss: 0.0431\nEpoch [10/20], Loss: 0.0285\nEpoch [11/20], Loss: 0.0258\nEpoch [12/20], Loss: 0.1933\nEpoch [13/20], Loss: 0.0316\nEpoch [14/20], Loss: 0.0399\nEpoch [15/20], Loss: 0.0546\nEpoch [16/20], Loss: 0.0661\nEpoch [17/20], Loss: 0.0291\nEpoch [18/20], Loss: 0.0105\nEpoch [19/20], Loss: 0.0390\nEpoch [20/20], Loss: 0.0536\n\n\n\n# 모델 평가\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n        outputs = model(inputs)\n        #print(outputs.data)\n        _, predicted = torch.max(outputs.data, 1)\n        #print(predicted)\n        total += labels.size(0)\n        labels_max = torch.max(labels, 1)[1]\n        #print(labels_max)\n        correct += (predicted == labels_max).sum().item()\n\n    print(f'Accuracy of the model on the test images: {100 * correct / total} %')\n\nAccuracy of the model on the test images: 97.75 %\n\n\n\n검증을 위한 데이터 저장, 훈련, 예측 동시 수행\n\n\nimport matplotlib.pyplot as plt\n\n# 데이터 저장을 위한 리스트 초기화\ntrain_losses = []\nval_accuracies = []\n\nnum_epochs = 200\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        if torch.cuda.is_available():\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    train_losses.append(epoch_loss)\n\n    # 모델 평가\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            if torch.cuda.is_available():\n                inputs, labels = inputs.cuda(), labels.cuda()\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            labels_max = torch.max(labels, 1)[1]\n            correct += (predicted == labels_max).sum().item()\n\n    epoch_accuracy = 100 * correct / total\n    val_accuracies.append(epoch_accuracy)\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n\n# 그래프 그리기\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.legend()\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Deep Learning</span>"
    ]
  }
]